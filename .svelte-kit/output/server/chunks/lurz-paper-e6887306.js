import { c as create_ssr_component } from "./app-1a513ba3.js";
import "@sveltejs/kit/ssr";
const metadata = {
  "title": "Generalization in data-driven models of primary visual cortex",
  "date": "2020-12-4",
  "description": "Discussion of Lurz et al., 2020 and tricks for learning deep stimulus models from neural data"
};
const Lurz_paper = create_ssr_component(($$result, $$props, $$bindings, slots) => {
  return `<blockquote><p>This is entry is the first in a <strong>journal club</strong> series, where I do a deep dive into a paper I\u2019ve recently gone over in a journal club or lab meeting.</p></blockquote>
<p>Today we\u2019re discussing <a href="${"https://www.biorxiv.org/content/10.1101/2020.10.05.326256v1"}" rel="${"nofollow"}">Lurz et al., 2020</a> from Fabian Sinz and colleagues. In it, they introduce a few new tricks for fitting CNNs end-to-end to neural data. <em>Then they show their data-driven model generalizes to predict the responses of neurons that it wasn\u2019t trained on (in animals that were not part of the training set!)</em></p>
<p>The main points of interest for me were:</p>
<ul><li>General conceptualization of \u201Creadout\u201D and \u201Ccore\u201D</li>
<li>\u201CTricks\u201D for learning the readout</li>
<li>Generalization performance</li></ul>
<h2 id="${"readout-and-core"}">\u201CReadout\u201D and \u201CCore\u201D</h2>
<p>Many groups have now presented advantages to thinking of neural population activity as resulting from shared computations with neuron-specific weights. Rather than optimize a model that predicts the responses of each neuron, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8879999999999999em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">r</span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8879999999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">i</span><span class="${"mclose mtight"}">)</span></span></span></span></span></span></span></span></span></span></span></span></span>, given the stimulus, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">x</span></span></span></span></span>, assume that each neuron operates with an affine transformation (\u201Creadout\u201D) of a core stimulus processing model that is shared by all neurons.</p>
<p>The simplest versions of this have a single linear filter as the core. For example, <a href="${"https://openreview.net/pdf?id=HkEI22jeg"}" rel="${"nofollow"}">Batty et al., 2017</a> used knowledge of the cell class to group retinal ganglion cells and learn a single space-time linear filter for all neurons of the same class. Their \u201Cmultitask LN\u201D model has a single filter for all On-RGCs and each neuron simply scales that filter\u2019s response with a neuron specific gain and offset term and then passes that through a sigmoid nonlinearity.</p>
<img src="${"/labmeetinglurz/battymultitaskln.png"}" style="${"max-width: 50em"}">
<p>I really like the terms <strong>\u201Ccore\u201D</strong> and <strong>\u201Creadout\u201D</strong> and think they should be adopted as the standard nomenclature for such a model. In the figure above, the <span style="${"color:blue"}">blue parameters</span> (<span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.83333em;vertical-align:-0.15em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.13889em;"}">W</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.2805559999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">t</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.83333em;vertical-align:-0.15em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.13889em;"}">W</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.151392em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">s</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span>) are shared by all On-RGCs and form the <strong>\u201Ccore\u201D</strong>. The <span style="${"color:green"}">green parameters</span> form the <strong>\u201Creadout\u201D</strong>.</p>
<p>In this case, the core is simple and interpretable (it\u2019s a single space-time separable linear filter). The readout is also simple. It\u2019s a gain and offset term per neuron. But this conceptual framing scales nicely to talking about much more complicated neural network models and nicely delineates their distinctions. But, every model consists of a \u201Ccore\u201D and a \u201Creadout\u201D.</p>
<h3 id="${"two-basic-types-of-cores"}">Two basic types of cores</h3>
<p>Once you accept the \u201Ccore\u201D and \u201Creadout\u201D distinction, cores have two basic distinctions in neuroscience research. They can either be <strong>\u201Cdata-driven\u201D</strong> or <strong>\u201Cgoal-directed\u201D</strong>.</p>
<p><strong>Goal-directed</strong> cores consist of a model that was trained to do <strong>some task</strong> given a stimulus (e.g., object classification). We\u2019ve seen this succussfully applied in neuroscience to a number cases, particularly in the ventral stream of primates (e.g., Yamins et al., ).</p>
<p><strong>Data-driven</strong> cores learn the core directly from the <strong>neural activity</strong>.</p>
<p>This idea is really jsut something that has been said by others already (e.g., <a href="${"http://neurotheory.umd.edu/Publications_files/ARVSpreprint.pdf"}" rel="${"nofollow"}">this paper from Dan Butts</a>), but I\u2019m converging on certain language for talking about it myself: \u201Cdata-driven core\u201D vs. \u201Cgoal-directed core\u201D.</p>
<p>The figure below demonstrates the logic of a data-driven core. It is trained end-to-end from stimulus to spikes. The core is shared by all neurons and the readout is neuron specific.</p>
<img src="${"/labmeetinglurz/buttsdatadriven.png"}" style="${"max-width: 50em"}">
<p>In contrast, a goal-directed core comes pre-trained (on some other dataset) and and forms a nonlinear basis for a linearized model of the neural responses.</p>
<img src="${"/labmeetinglurz/buttstransfer.png"}" style="${"max-width: 50em"}">
<p><strong>The advantages of a goal-directed core are:</strong></p>
<ol><li>they can use much more data than is typically available in a neural recording</li>
<li>they have an explicit task, so they provide convenient language for talking about what the core <em>does</em></li></ol>
<p><strong>The advantages (hopes) of data-driven cores are:</strong></p>
<ol><li>they nonlinearities that are brain specific (opposed to input-specific or task-specific)</li>
<li>can be constrained with brain-inspired architecture</li></ol>
<p>There are disadvantages to both approaches as well. The primary disadvantage of goal-directed cores in vision is that you\u2019re mostly stuck with whatever the ML community has been most excited about. I think this has had an unfortunate side effect of pushing more of visual neuroscience into studying the responses to images (because that\u2019s what the models can do). Of course, goal-directed cores can also be constrained with brain-like architecture and trained from scratch, and we\u2019ll probably see more of that happening, but then you\u2019re back dealing with data/computation limits. Another limitation of a goal-directed network is that you have to know what the neurons do <em>a priori</em> instead of just knowing what their inputs are. What does the retina do?</p>
<p>Okay. Now that we\u2019re all on the same page, a real test of a data-driven core is whether it can generalize like goal-directed cores do. Goal-directed cores are generalize from the task they were trained to perform to predict neural activity by training the readout. Lurz and colleagues do the same thing here. Train the core and readout on one set of neurons, then fix the core and train only the readout on another set of neurons.</p>
<h2 id="${"old-tricks-for-learning-the-readout"}">Old \u201CTricks\u201D for learning the readout</h2>
<p>For a typical convolutional neural network (CNN), the per-neuron readout scales with the size of the input and the number of channels. For example, the activations of the final layer of a CNN, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">a</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">F</span></span><span class="${"mopen"}">[</span><span class="${"mord mathnormal"}">x</span><span class="${"mclose"}">]</span></span></span></span></span>, where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">x</span></span></span></span></span> is an image of size <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.66666em;vertical-align:-0.08333em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.69444em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">h</span></span></span></span></span>, is <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.5782em;vertical-align:-0.0391em;"}"></span><span class="${"mord mathnormal"}">a</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u2208</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8491079999999999em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">R</span></span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8491079999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mbin mtight"}">\xD7</span><span class="${"mord mathnormal mtight"}">h</span><span class="${"mbin mtight"}">\xD7</span><span class="${"mord mathnormal mtight"}">c</span></span></span></span></span></span></span></span></span></span></span></span></span> where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> is the number of channels in the network.</p>
<p>For the models the authors are considering here, the image size is <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">6</span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">3</span><span class="${"mord"}">6</span></span></span></span></span> so there are 2304 parameters per output channel per neuron! Even with structured regularization (smoothness, sparseness), this is a big problem to fit in a normal dataset.</p>
<p>There have been a series of \u201Ctricks\u201D for learning the readout that these authors have rolled out over the last few years.</p>
<h3 id="${"trick-1-factorized-readout-klindtecker-et-al-2016"}">Trick <a href="${"https://github.com/jcbyts/website/issues/1"}">#1</a>: Factorized readout (Klindt,Ecker et al., 2016)</h3>
<p>The first trick is to learn the same spatial readout for all channels in the model. This separates \u201Cwhat\u201D features the neuron integrates from \u201Cwhere\u201D the neurons are spatially selective.</p>
<p>The activations, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">a</span></span></span></span></span>, are size <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mopen"}">[</span><span class="${"mord mathnormal"}" style="${"margin-right:0.10903em;"}">N</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.66666em;vertical-align:-0.08333em;"}"></span><span class="${"mord mathnormal"}">c</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.66666em;vertical-align:-0.08333em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord mathnormal"}">h</span><span class="${"mclose"}">]</span></span></span></span></span>, where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.68333em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.10903em;"}">N</span></span></span></span></span> is the batch size, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> is the channels, and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.66666em;vertical-align:-0.08333em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.69444em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">h</span></span></span></span></span> are the width and height of the input images. The activations are multiplied by a feature vector, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.9270999999999999em;vertical-align:-0.0391em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8879999999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">i</span><span class="${"mclose mtight"}">)</span></span></span></span></span></span></span></span></span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u2208</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.68889em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">R</span></span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.664392em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">c</span></span></span></span></span></span></span></span></span></span></span></span>, and a spatial weight vector, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.9270999999999999em;vertical-align:-0.0391em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">v</span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8879999999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">i</span><span class="${"mclose mtight"}">)</span></span></span></span></span></span></span></span></span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u2208</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8491079999999999em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">R</span></span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8491079999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mbin mtight"}">\xD7</span><span class="${"mord mathnormal mtight"}">h</span></span></span></span></span></span></span></span></span></span></span></span></span>, where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.65952em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">i</span></span></span></span></span> is the neuron index.</p>
<p>Therefore the response of neuron <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.65952em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">i</span></span></span></span></span> will be</p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8879999999999999em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">r</span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8879999999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">i</span><span class="${"mclose mtight"}">)</span></span></span></span></span></span></span></span></span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.4822159999999998em;vertical-align:-0.4374159999999999em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.13889em;"}">F</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"delimsizing size1"}">[</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mop"}"><span class="${"mop op-symbol small-op"}" style="${"position:relative;top:-0.0000050000000000050004em;"}">\u2211</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.18639799999999984em;"}"><span style="${"top:-2.40029em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">h</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">c</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.43581800000000004em;"}"><span></span></span></span></span></span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:1.0448em;"}"><span style="${"top:-2.5834080000000004em;margin-left:-0.02691em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">c</span></span></span></span><span style="${"top:-3.2198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">i</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.11659199999999997em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">v</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:1.0448em;"}"><span style="${"top:-2.3986920000000005em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">h</span></span></span></span><span style="${"top:-3.2198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">i</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.4374159999999999em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"mord mathnormal"}">a</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">c</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02691em;"}">w</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">h</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"delimsizing size1"}">]</span></span></span></span></span></span></p>
<p>where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.83333em;vertical-align:-0.15em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.13889em;"}">F</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> is the activation function for neuron <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.65952em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">i</span></span></span></span></span>, which is an <a href="${"https://paperswithcode.com/method/elu"}" rel="${"nofollow"}">ELU</a> with an offset.</p>
<img src="${"/labmeetinglurz/elu.png"}" style="${"max-width: 20em"}">
<p>This reduces the number of parameters from <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mord"}">3</span><span class="${"mord"}">0</span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> to <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mord"}">3</span><span class="${"mord"}">0</span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span></p>
<h3 id="${"trick-2-coordinate-readout-using-bilinear-interpolation-learned-with-pooling-steps-sinz-et-al-2018"}">Trick <a href="${"https://github.com/jcbyts/website/issues/2"}">#2</a>: coordinate readout using bilinear interpolation, learned with pooling steps (Sinz et al., 2018)</h3>
<p>This approach assumes that each neuron has a feature vector that reads out from a spatial position (a single point) in the spatial output of the network.</p>
<p>The spatial position for each neuron, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">x</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> are learned parameters. They are sampled at sub-pixel resolution using bilinear interpolation. The issue with learning <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">x</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> is that if the initialization is far away from the true retinotopic location of the neuron, then the gradients will be zero. To circumvent this, the authors represent the feature space of the core at multiple scales using <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">4</span></span></span></span></span> average pooling steps with <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">4</span></span></span></span></span> pooling with a stride of <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">4</span></span></span></span></span>, such that the final stage is a single pixel. They then learn a feature vector that combines across these scales. <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">x</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> can be any value within a <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">1</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">1</span></span></span></span></span> feature space and that way there are gradients to support the learning of the features and the position.</p>
<p>This readout idea comes from <a href="${"https://arxiv.org/abs/1506.02025"}" rel="${"nofollow"}">spatial transformer layers</a>. The basic transform operation is an affine transform of a grid of sampling points.</p>
<img src="${"/labmeetinglurz/spatialtransformer.png"}" style="${"max-width: 50em"}">
<p>The difference here is that the sample is a single point for each neuron and it is sampling that point in a coordinate system that is the same regardless of the pooling size. That way the initialization always has something to start with.</p>
<img src="${"/labmeetinglurz/pointreadout.png"}" style="${"max-width: 50em"}">
<p><strong>This has two cool benefits:</strong></p>
<ol><li><p>The number of parameters is reduced from <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mord"}">3</span><span class="${"mord"}">0</span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> to <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">m</span><span class="${"mord mathnormal"}">c</span></span></span></span></span>, where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">m</span></span></span></span></span> is the number of pooling steps, and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> is the number of channels.</p></li>
<li><p><strong>Eyetracking!</strong> Because the readout is parameterized as an x,y coordinate, they can shift the entire readout around with a \u201Cshifter network\u201D that operates on the pupil position in a video of the eye.</p></li></ol>
<p>The full model in the 2018 paper is schematized here:</p>
<img src="${"/labmeetinglurz/sinz2018.png"}" style="${"max-width: 30em"}">
<p>Okay, so now that we have a sense of the readout, we\u2019re ready for the new tricks introduced in Lurz et al., 2020.</p>
<h2 id="${"new-tricks-for-learning-the-readout"}">New Tricks for learning the readout</h2>
<p>Using the same bilinear interpolation readout from Sinz et al., 2018, the authors improve the learning of the <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">x</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> for each neuron. They do so by using the \u201Creparameterization trick\u201D (Kingma and Welling)</p>
<h3 id="${"quick-refresher-on-vaes"}">Quick refresher on VAEs</h3>
<p>We\u2019ve discussed VAEs in lab meeting in the past, so we already learned the tricks that we need here to learn this new readout. This section here is a really abridged reminder on variation autoencoders with emphasis on the \u201Creparameterization trick\u201D as it will be applied. If you care at all about details, read (<a href="${"https://arxiv.org/pdf/1906.02691.pdf"}" rel="${"nofollow"}">this</a>) nice tutorial.</p>
<p>Start with a generative model of data <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.5782em;vertical-align:-0.0391em;"}"></span><span class="${"mord mathnormal"}">x</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u2208</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8413309999999999em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">R</span></span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8413309999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">D</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}">x</span><span class="${"mclose"}">)</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.11112em;vertical-align:-0.30612em;"}"></span><span class="${"mop op-symbol small-op"}" style="${"margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;"}">\u222B</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}">x</span><span class="${"mord"}">\u2223</span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mclose"}">)</span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mclose"}">)</span><span class="${"mord mathnormal"}">d</span><span class="${"mord mathnormal"}">x</span></span></span></span></span></span></p>
<p>where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.5782em;vertical-align:-0.0391em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u2208</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8491079999999999em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">R</span></span><span class="${"msupsub"}"><span class="${"vlist-t"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.8491079999999999em;"}"><span style="${"top:-3.063em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">d</span><span class="${"mrel mtight"}">\u226A</span><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">D</span></span></span></span></span></span></span></span></span></span></span></span></span> is a latent space and achieves some dimensionality reduction.</p>
<p>There are two tricks to learning the posterior <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mord"}">\u2223</span><span class="${"mord mathnormal"}">x</span><span class="${"mclose"}">)</span></span></span></span></span>:</p>
<p>First, we approximated an intractible posterior with a variational distribution <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">q</span></span></span></span></span> and we showed that we only needed to maximize the ELBO to fit the parameters <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.69444em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span> and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span>.</p>
<p>Second, we reparameterized the loss to take the sampling step outsize of the optimization. This reparameterization trick let us take the gradients with respect to the parameters we\u2019re interested in fitting. It\u2019s the same trick they use to do the sampling here.</p>
<p>One final reminder about VAEs is that this is a generative modeling approach with Bayesian inference for the latents, but it can also be referred to in coding theory terms like <em>Endoding</em> and <em>Decoding</em>.</p>
<p>The basic idea is pictured here</p>
<img src="${"/labmeetinglurz/kingmavaelearning.png"}" style="${"max-width: 30em"}">
<p>I\u2019m going to skip derivations and you can look at the link above if you want them. The key point is that by starting with the objective of maximizing the marginal likelihood <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}">x</span><span class="${"mclose"}">)</span></span></span></span></span> you end up with two terms in the loss: one that is a KL divergence between the posterior approximation <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mord"}">\u2223</span><span class="${"mord mathnormal"}">x</span><span class="${"mclose"}">)</span></span></span></span></span> and the true (intractable) posterior <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">p</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.33610799999999996em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mord"}">\u2223</span><span class="${"mord mathnormal"}">x</span><span class="${"mclose"}">)</span></span></span></span></span> and another known as the Evidence Lower Bound (ELBO) that I\u2019m showing here:</p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathcal"}">L</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.2332799999999997em;vertical-align:-0.38327999999999984em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">E</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3448em;"}"><span style="${"top:-2.5198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3448em;"}"><span style="${"top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"}"><span class="${"pstrut"}" style="${"height:2.5em;"}"></span><span class="${"sizing reset-size3 size1 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.29011428571428566em;"}"><span></span></span></span></span></span></span><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mord mtight"}">\u2223</span><span class="${"mord mathnormal mtight"}">x</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.38327999999999984em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"delimsizing size1"}">[</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mop"}">lo<span style="${"margin-right:0.01389em;"}">g</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">p</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.33610799999999996em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\u2212</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.20001em;vertical-align:-0.35001em;"}"></span><span class="${"mop"}">lo<span style="${"margin-right:0.01389em;"}">g</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mord"}">\u2223</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span><span class="${"mord"}"><span class="${"delimsizing size1"}">]</span></span></span></span></span></span></p>
<p>I\u2019m not showing the KL term here because, due to the non-negativity of the KL divergence, the ELBO is a lower bound on the log-likelihood of the data.</p>
<p>We want to maximize the ELBO, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathcal"}">L</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span></span></span></span></span>, w.r.t. the parameters <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.69444em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span> and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span>, because this approximately maximizes the marginal likelihood and minimizes the KL divergence of the approximation to the true posterior.</p>
<p>Maximizing the the ELBO w.r.t. <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.69444em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span> is straightforward because the expectation is take w.r.t. the distribution <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mord"}">\u2223</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span></span></span></span></span> so we can move the gradient operator inside the expectation.</p>
<img src="${"/labmeetinglurz/wrttheta.png"}" style="${"max-width: 30em"}">
<p>Maximizing the the ELBO w.r.t. <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span> is tricky because the expectation is w.r.t. the distribution <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mord"}">\u2223</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span></span></span></span></span>, which is a function of <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span>.</p>
<img src="${"/labmeetinglurz/wrtphi.png"}" style="${"max-width: 30em"}">
<p>This is where the \u201Creparameterization trick\u201D comes in. <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathcal"}">L</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span></span></span></span></span> can be differentiated w.r.t. <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span> with a change of variables.</p>
<p>First, express the random variable <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u223C</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.036108em;vertical-align:-0.286108em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mord"}">\u2223</span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span></span></span></span></span> as a differentiable and invertable transformation of another random variable <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">\u03F5</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u223C</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord"}"><span class="${"mord mathcal"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen"}">(</span><span class="${"mord"}">0</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">I</span></span><span class="${"mclose"}">)</span></span></span></span></span>,<span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span>, and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span></span></span></span></span></p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">g</span></span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}">\u03F5</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mclose"}">)</span></span></span></span></span></p>
<p>With this change of variable, the expectation can be written w.r.t. the <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">\u03F5</span></span></span></span></span> and the gradient can move inside the expectation.</p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.1332799999999998em;vertical-align:-0.38327999999999984em;"}"></span><span class="${"mord"}"><span class="${"mord"}">\u2207</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">E</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3448em;"}"><span style="${"top:-2.5198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.03588em;"}">q</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3448em;"}"><span style="${"top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"}"><span class="${"pstrut"}" style="${"height:2.5em;"}"></span><span class="${"sizing reset-size3 size1 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.29011428571428566em;"}"><span></span></span></span></span></span></span><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mord mtight"}">\u2223</span><span class="${"mord mathnormal mtight"}">x</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.38327999999999984em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">[</span><span class="${"mord mathnormal"}" style="${"margin-right:0.10764em;"}">f</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.04398em;"}">z</span><span class="${"mclose"}">)</span><span class="${"mclose"}">]</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.1052em;vertical-align:-0.3551999999999999em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">E</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.34480000000000005em;"}"><span style="${"top:-2.5198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03F5</span><span class="${"mrel mtight"}">\u223C</span><span class="${"mord mtight"}"><span class="${"mord mathcal mtight"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen mtight"}">(</span><span class="${"mord mtight"}">0</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mtight"}">1</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3551999999999999em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">[</span><span class="${"mord"}"><span class="${"mord"}">\u2207</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3361079999999999em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03D5</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.286108em;"}"><span></span></span></span></span></span></span><span class="${"mord mathnormal"}" style="${"margin-right:0.10764em;"}">f</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">g</span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span><span class="${"mclose"}">)</span><span class="${"mclose"}">)</span><span class="${"mclose"}">]</span></span></span></span></span></p>
<p>This reparameterization means <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.10764em;"}">f</span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">g</span><span class="${"mopen"}">(</span><span class="${"mord text"}"><span class="${"mord textrm"}">z</span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">x</span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span><span class="${"mclose"}">)</span><span class="${"mclose"}">)</span></span></span></span></span> can be automatically differentiated w.r.t. the parameters <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span> using whatever software is your current favorite (Tensorflow, Pytorch).</p>
<p>The schematic that illustrates this can be seen here:</p>
<img src="${"/labmeetinglurz/kingmavaereparameterization.png"}" style="${"max-width: 30em"}">
<h2 id="${"new-trick-1-reparameterization--sampling"}">New Trick <a href="${"https://github.com/jcbyts/website/issues/1"}">#1</a>: reparameterization + sampling</h2>
<p>The model still uses the poisson loss, but now it depends on random variables <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">x</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u223C</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord"}"><span class="${"mord mathcal"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen"}">(</span><span class="${"mord"}"><span class="${"mord mathnormal"}">\u03BC</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord"}">\u03A3</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mclose"}">)</span></span></span></span></span>, where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8777699999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">\u03BC</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord"}">\u03A3</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.31166399999999994em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mathnormal mtight"}">i</span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span></span></span></span></span> are learned parameters.</p>
<p>The new loss function involves an expectation over the distribution of <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">x</span></span></span></span></span> and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span></span></span></span></span>.</p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord"}"><span class="${"mord mathcal"}">L</span></span><span class="${"mopen"}">(</span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mpunct"}">;</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">r</span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">x</span></span></span><span class="${"mclose"}">)</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.1052em;vertical-align:-0.3551999999999999em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">E</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.34480000000000005em;"}"><span style="${"top:-2.5198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">x</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"mrel mtight"}">\u223C</span><span class="${"mord mtight"}"><span class="${"mord mathcal mtight"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">\u03BC</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mtight"}">\u03A3</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3551999999999999em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">[</span><span class="${"mop"}">lo<span style="${"margin-right:0.01389em;"}">g</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">r</span></span></span><span class="${"mord"}">\u2223</span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">x</span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mclose"}">)</span><span class="${"mclose"}">]</span></span></span></span></span></p>
<p>where <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.44444em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">r</span></span></span></span></span></span></span> is the observed spike counts, <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.44444em;vertical-align:0em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">x</span></span></span></span></span></span></span> is the stimulus, and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.69444em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord"}"><span class="${"mord mathnormal"}">\u03D5</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">\u03BC</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord"}">\u03A3</span></span></span></span></span></span> are the parameters of the model. <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03BC</span></span></span></span></span> and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.68333em;vertical-align:0em;"}"></span><span class="${"mord"}">\u03A3</span></span></span></span></span> are the mean and variance of a multivariate Gaussian that generates the <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">x</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span></span></span></span></span> coordinates for the neuron readouts, and <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.8888799999999999em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03D5</span></span></span></span></span> is all other parameters in the CNN.</p>
<p>We can use the same reparameterization trick from above to make the gradients easy to compute. Make <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">x</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.03588em;"}">y</span></span></span></span></span> some function of a new random variable <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">\u03F5</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u223C</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord"}"><span class="${"mord mathcal"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen"}">(</span><span class="${"mord"}">0</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">I</span></span><span class="${"mclose"}">)</span></span></span></span></span>, then the expectation is over <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">\u03F5</span></span></span></span></span> and we can move the gradient operator inside the expectation.</p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:1.1052em;vertical-align:-0.3551999999999999em;"}"></span><span class="${"mord"}"><span class="${"mord"}">\u2207</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.33610799999999996em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">E</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.34480000000000005em;"}"><span style="${"top:-2.5198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">x</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.03588em;"}">y</span><span class="${"mrel mtight"}">\u223C</span><span class="${"mord mtight"}"><span class="${"mord mathcal mtight"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen mtight"}">(</span><span class="${"mord mathnormal mtight"}">\u03BC</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mtight"}">\u03A3</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3551999999999999em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">[</span><span class="${"mop"}">lo<span style="${"margin-right:0.01389em;"}">g</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">r</span></span></span><span class="${"mord"}">\u2223</span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">x</span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mclose"}">)</span><span class="${"mclose"}">]</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">=</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1.1052em;vertical-align:-0.3551999999999999em;"}"></span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbb"}">E</span></span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.34480000000000005em;"}"><span style="${"top:-2.5198em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}">\u03F5</span><span class="${"mrel mtight"}">\u223C</span><span class="${"mord mtight"}"><span class="${"mord mathcal mtight"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen mtight"}">(</span><span class="${"mord mtight"}">0</span><span class="${"mpunct mtight"}">,</span><span class="${"mord mtight"}">1</span><span class="${"mclose mtight"}">)</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.3551999999999999em;"}"><span></span></span></span></span></span></span><span class="${"mopen"}">[</span><span class="${"mord"}"><span class="${"mord"}">\u2207</span><span class="${"msupsub"}"><span class="${"vlist-t vlist-t2"}"><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.33610799999999996em;"}"><span style="${"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"}"><span class="${"pstrut"}" style="${"height:2.7em;"}"></span><span class="${"sizing reset-size6 size3 mtight"}"><span class="${"mord mtight"}"><span class="${"mord mathnormal mtight"}" style="${"margin-right:0.02778em;"}">\u03B8</span></span></span></span></span><span class="${"vlist-s"}">\u200B</span></span><span class="${"vlist-r"}"><span class="${"vlist"}" style="${"height:0.15em;"}"><span></span></span></span></span></span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mop"}">lo<span style="${"margin-right:0.01389em;"}">g</span></span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">p</span><span class="${"mopen"}">(</span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">r</span></span></span><span class="${"mord"}">\u2223</span><span class="${"mord"}"><span class="${"mord"}"><span class="${"mord mathbf"}">x</span></span></span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}" style="${"margin-right:0.02778em;"}">\u03B8</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord mathnormal"}">\u03F5</span><span class="${"mclose"}">)</span><span class="${"mclose"}">]</span></span></span></span></span></p>
<p>In practice, all you have to do is sample 1 draw from <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">\u03F5</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span><span class="${"mrel"}">\u223C</span><span class="${"mspace"}" style="${"margin-right:0.2777777777777778em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:1em;vertical-align:-0.25em;"}"></span><span class="${"mord"}"><span class="${"mord mathcal"}" style="${"margin-right:0.14736em;"}">N</span></span><span class="${"mopen"}">(</span><span class="${"mord"}">0</span><span class="${"mpunct"}">,</span><span class="${"mspace"}" style="${"margin-right:0.16666666666666666em;"}"></span><span class="${"mord text"}"><span class="${"mord textrm"}">I</span></span><span class="${"mclose"}">)</span></span></span></span></span> for each sample in a batch during regular old SGD.</p>
<p>This has a huge reduction in the number of parameters in the readout. We went from <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mord"}">3</span><span class="${"mord"}">0</span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">\xD7</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> in the full space to <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mord"}">3</span><span class="${"mord"}">0</span><span class="${"mord"}">4</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">c</span></span></span></span></span> in the <em>factorized</em> case. Then we made it down to <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.72777em;vertical-align:-0.08333em;"}"></span><span class="${"mord"}">2</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.43056em;vertical-align:0em;"}"></span><span class="${"mord mathnormal"}">m</span><span class="${"mord mathnormal"}">c</span></span></span></span></span> in the <em>coordinate + pooling</em> case. With this final innovation, we\u2019re down to <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.66666em;vertical-align:-0.08333em;"}"></span><span class="${"mord mathnormal"}">c</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">6</span></span></span></span></span> parameters per neuron (not including the bias \u2013 which I didn\u2019t include in any of the other parameter counts)!!</p>
<p><span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.66666em;vertical-align:-0.08333em;"}"></span><span class="${"mord mathnormal"}">c</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span><span class="${"mbin"}">+</span><span class="${"mspace"}" style="${"margin-right:0.2222222222222222em;"}"></span></span><span class="${"base"}"><span class="${"strut"}" style="${"height:0.64444em;vertical-align:0em;"}"></span><span class="${"mord"}">7</span></span></span></span></span> total parameters per neuron to learn for the readout should not be hard, but they don\u2019t stop there. They use one more trick.</p>
<h3 id="${"new-trick-2-retinotopy"}">New Trick <a href="${"https://github.com/jcbyts/website/issues/2"}">#2</a>: retinotopy</h3>
<p>Neurons in V1 are organized spatially such that cortical space maps onto visual space. This is known as retinotopy because cortical space forms a map that is in retinal (and therefore visual) coordinates.</p>
<p>Using this additional information, the authors learn a mapping from cortical space (where they measured the location of the neurons) to the <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03BC</span></span></span></span></span> parameter. This reduced the total number of parameters per neuron by 2 and makes shifts in <span class="${"math math-inline"}"><span class="${"katex"}"><span class="${"katex-html"}" aria-hidden="${"true"}"><span class="${"base"}"><span class="${"strut"}" style="${"height:0.625em;vertical-align:-0.19444em;"}"></span><span class="${"mord mathnormal"}">\u03BC</span></span></span></span></span> shared.</p>
<p>Figure 2 in the paper illustrates this conceptually.</p>
<img src="${"/labmeetinglurz/lurzfig2.png"}" style="${"max-width: 40em"}">
<p>Now they\u2019re really showing off! With all of these new tricks in place, they are ready to train cores and test them on witheld datasets\u2026 yea, witheld <em>datasets</em>, not just witheld <em>data</em>.</p>
<h3 id="${"generalization-and-performance"}">Generalization and Performance</h3>
<p>The rest of the paper just shows how well these different readouts work for different sets of training and test data. I don\u2019t have much to say except the new readout easily beats the state-of-the-art. And it generalizes better than VGG-16 as a core model.</p>
<img src="${"/labmeetinglurz/lurzfig3.png"}" style="${"max-width: 30em"}">
<p><strong>Figure 3:</strong> Performance of end-to-end trained networks. Performance for different subsets of neurons (linestyle) and number of training examples (x-axis). The same core architecture was trained for two different readouts with and without feature sharing (color) on the matched neurons of the 4-S:matched core set (Fig. 1, green). Both networks show increasing performance with number of images However, the network with the Gaussian readout achieves a higher final performance (light blue vs. orange). While the Gaussian readout profits from feature sharing (light vs. dark blue), the factorized readout is hurt by it (yellow vs. orange). Shaded areas depict 95% confidence intervals across random picks of the neuron subsets</p>
<img src="${"/labmeetinglurz/lurzfig4.png"}" style="${"max-width: 60em"}">
<p><strong>Figure 4:</strong> Generalization to other neurons in the same animal. A core trained on 3597 neurons and up to 17.5k images generalizes to new neurons (pink and yellow line). A fully trained core yields very good predictive performance even when the readout is trained on far less data (yellow). If the readout is trained with all data, even a sub-optimal core can yield a good performance (pink). Both transfer conditions outperform a network directly trained end-to-end on the transfer dataset (brown). For the full dataset, all training conditions converge to the same performance. Except in the best-core/diff-readout condition for very few training data, the Gaussian readout (B) outperforms the factorized readout (A). The data for both the training and transfer comes from the 4-S:matched dataset (Fig 1, green). Not that the different number of images can be from the core or transfer set, depending on the transfer condition.</p>
<img src="${"/labmeetinglurz/lurzfig5.png"}" style="${"max-width: 30em"}">
<p><strong>Figure 5:</strong> Generalization across animals. Prediction performance in fraction oracle correlation as a function of
training examples in the transfer set for
a Gaussian readout (x-axis) and different ways to obtain the core (colors).
The transfer training was performed
on the evaluation dataset (blue, Fig 1).
Cores trained on several scans used
in transfer learning outperform direct
training on the transfer dataset (blue
line; direct condition).</p>
<h3 id="${"discussion-and-thoughts"}">Discussion and thoughts</h3>
<p>Overall, this is really impressive. But, I\u2019m still left wishing these guys would do some science! Haha. Joking aside, it is nice that the ML conference format means we\u2019re all up to date on what tricks they\u2019re learning to fit these models, but there haven\u2019t really been any real scientific insights from this series of papers, besides maybe the <a href="${"https://www.biorxiv.org/content/10.1101/767285v4"}" rel="${"nofollow"}">divisive normalization paper</a>. Even the <a href="${"https://xaqlab.com/wp-content/uploads/2019/11/Inception_Walker_plusSupp.pdf"}" rel="${"nofollow"}">\u201CInception\u201D</a> paper was really underwhelming. All of that effort to find things that look like gabors with surrounds and don\u2019t really drive V1 neurons much better. I\u2019d say we\u2019re still learning much more interesting things about mouse visual cortex <a href="${"https://nbdt.scholasticahq.com/article/5123-on-the-subspace-invariance-of-population-responses"}" rel="${"nofollow"}">using gratings</a>, which is a real disappointment for the \u201Cstate-of-the-art\u201D. Of course, this is a high bar for a subfield that is so new, but I would be really disappointed if the ML business of focusing on perfomance spreads into neuroscience more than <a href="${"http://www.brain-score.org/"}" rel="${"nofollow"}">it already has</a>. Yes, performance is important, but that alone is not the goal and performance obviously has to be mixed with insights (e.g., <a href="${"https://issalab.neuroscience.columbia.edu/sites/default/files/content/Kar%20et%20al.%20Evidence%20that%20recurrent%20circuits_Nat%20Neuro%202019.pdf"}" rel="${"nofollow"}">Kar et al., 2019</a>) <a href="${"https://www.biorxiv.org/content/10.1101/767285v4"}" rel="${"nofollow"}">Burg et al., 2020</a> is definitely a step in the right direction!</p>
<p>I\u2019d love to see anatomically constrained cores and attempts to explain nonlinear responses in V1 parsimoniously (<a href="${"https://arxiv.org/abs/1912.06207"}" rel="${"nofollow"}">like this</a>). There are other ways this type of model could be useful. Often neuroscientists do not care about the stimulus processing model, but simply care that they have a way to modulate and predict responses so they can test for attentional modulations or decision signals. I\u2019d like to see this framework applied to a a more natural task: the shifter and modulator networks in Sinz et al., 2018 provide a perfect vehicle to ask these types of questions.</p>
<p>Anyway, Lurz et al., 2020 is a very impressive method with a lot of clever tricks in it. Looking forward to the next in the series!</p>`;
});
export { Lurz_paper as default, metadata };
