---
title: AI for Science
date: 2025-9-30
description: Three paths for how AI will revolutionize scientific discovery and understanding
draft: false
hideDefaultHeader: true
---

<script>
  import PlayingCards from '$lib/components/PlayingCards.svelte';
  import ScenarioCards from '$lib/components/ScenarioCards.svelte';
</script>

<style>
  /* Article container styling */
  .article-wrapper {
    position: relative;
    --article-padding-x: 3.5rem;
    margin: -3.5rem -3.5rem -4rem -3.5rem;
    max-width: none;
  }

  /* Header section */
  .article-header {
    padding: 10rem 2rem 6rem;
    text-align: center;
    position: relative;
    border: 1px solid #e5e7eb;
    border-bottom: none;
  }

  .article-meta {
    font-size: 0.875rem;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: #10b981;
    margin-bottom: 1.5rem;
    font-weight: 500;
    font-family: "Inter", system-ui, -apple-system, sans-serif;
  }

  .article-title {
    font-size: 5rem;
    font-style: italic;
    font-weight: 300;
    letter-spacing: -0.02em;
    margin: 0 0 1rem 0;
    line-height: 1.1;
  }

  .article-description {
    font-size: 1.2rem;
    color: #999;
    font-weight: 300;
    margin: 0 auto;
    line-height: 1.5;
    font-family: "Inter", system-ui, -apple-system, sans-serif;
  }

  /* Content wrapper with borders */
  .content-wrapper {
    position: relative;
    display: grid;
    grid-template-columns: var(--article-padding-x) 1fr var(--article-padding-x);
    margin: 0 auto;
    padding: 3rem 0;
    background: white;
    border: 1px solid #e5e7eb;
  }

  .content-wrapper > * {
    grid-column: 2;
  }

  .content-wrapper .grid-full {
    grid-column: 1 / 4;
  }

  /* Section dividers */
  .section-divider {
    height: 1px;
    background: #e5e7eb;
    margin: 2.5rem -3.5rem;
  }

  /* Path section headers */
  .path-section {
    margin-bottom: 1.5rem;
  }

  .path-label {
    font-size: 0.875rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.15em;
    color: #6b7280;
    margin: 0 0 0.75rem 0;
    font-family: system-ui, -apple-system, sans-serif;
  }

  .path-section h3 {
    margin: 0;
    font-size: 1.875rem;
    font-weight: 600;
    color: #111827;
  }

  @media (max-width: 1024px) {
    .article-wrapper {
      --article-padding-x: 2rem;
    }
  }

</style>

<div class="article-wrapper">
  <div class="article-header">
    <h1 class="article-title">AI for Science</h1>
    <p class="article-description">Three paths for how AI will revolutionize scientific discovery and understanding</p>
  </div>

  <div class="content-wrapper">
    <p>In the next 10 years, AI is going to play a huge role in driving major breakthroughs in science. That’s ultimately good for humanity. "AI for science" is not a new idea, but it means different things to different people. I see three main paths:</p>

    <div class="grid-full mt-9">
        <PlayingCards />
    </div>

<div class="path-section pt-16">
<h6 class="path-label">PATH ONE</h6>
<h3 class="font-sans font-bold tracking-tight">Science as Optimization</h3>
</div>

This is the most common version I've heard. Just let AI solve the problem. This is the [AlphaFold](https://deepmind.google/science/alphafold/) path: Take your scientific question, reformulate it as an optimization problem, collect a ton of data, pray to the Bitter lesson gods, and hope your AI can solve it.

This path will lead to major medical breakthroughs. I have no doubt about this. But I’m not *that* excited about it for science. This path kind of cuts the human scientist out of the loop and it doesn't really lead in any direct way to *understanding*. It is exciting for humanity at large, but it's not really my favorite. From my point of view, Science is in the business of *why* things are. Why is the sky blue? Why does this bright light appear in the sky at this time of year? For that, we want [parsimonious explanations](https://www3.itp.tu-berlin.de/fileadmin/a3233/upload/SS07/MMP/WignerNobelLect_01.pdf). That requires theory, theory that humans can interpret.

I suspect most practicing scientists are not that excited about this path either. With science as optimization, practicing scientists exist to collect and format data for AI. A subset have already embraced this idea and many are working on how to scale up data collection and model fitting. In neuroscience, we already do versions of this—predicting neural activity given measured behavior and sensory signals. There are already efforts underway to build large [foundation models of neural activity](https://www.nature.com/articles/s41586-025-08829-y). I'm on board. Let's get the big foundation models for science. This will lead to medical breakthroughs, but I don't think it's the most exciting path forward for the human endeavor we call "science".

<div class="section-divider"></div>

<div class="path-section pt-16">
<h6 class="path-label">PATH TWO</h6>
<h3 class="font-sans font-bold tracking-tight">AI-driven Discovery</h3>
</div>

An althernate path is that AI will lead the way in generating new (theoretical) discoveries. To borrow another Deepmind model, this is the AlphaGo path. AlphaGo is reinforcement learning model trained to play the game of Go. It beat Lee Sedol in 2016, and famously utilized a move ([move 37](https://x.com/karpathy/status/1884336943321997800?lang=en)) that no human would've ever made. What is exciting about this direction is not that AI became the best Go player. What is exciting is that human play got better after AlphaGo.

Go is an ancient game. People had been playing it for centuries, and by many accounts, human performance had plateaued. Then move 37 came along and human players had to rethink the game. It created new gradients for humans to learn. This effect was quantified [here](https://www.pnas.org/doi/10.1073/pnas.2214840120) and it really highlights that an AI thinking "outside the box" can unlock paradigm shifts in otherwise stagnant fields of human study (here, Go). This is incredibly exciting. Think about all the fields where progress has slowed. AI could approach a scientific problem entirely differently, make a move we would never consider, and suddenly the entire landscape shifts. That’s a real breakthrough in understanding.

<img src="/aiforscience/goDQI.png" class="" style="max-width: min(100%, 22em)" alt="Human Go moves improved after AlphaGo" />
<p class="mt-3 text-xs text-gray-500 text-center max-w-[50em] mx-auto font-sans mb-16">Human Go moves improved in quality after AlphaGo. At a minimum they changed. The AI didn't just beat human players, it unlocked new gradients for learning among humans. This is the kind of paradigm shifting breakthrough that can happen with AI-dreiven discovery.</p>

I'm excited about this path, but it has two obstacles. One is sociological and the other is technical. The sociological obstacle is that scientists will not like being "led" by AI. This sounds too much like "replacement". We don't just want to be technicians in a lab run by AIs, at least most scientists won't. I think it is exciting that the humans will improve through a partnership with the AI, but I don't think many will see it that way.

The technical obstacle is that we don't have the right tools to train AI to make these kinds of discoveries. Notably, unlike it's successor, [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero), AlphaGo was pre-trained with human go moves before moving on to self-play in an RL loop. I suspect this second stage was pretty important and that move 37 never would've happened if it was only trained on human moves. Importantly, AlphaGo was able to learn through action (not just from predicting human moves). Until we let AIs run science labs in the real world and update their knowledge, this is probably unlikely in the short term.  But I have some hope that using an LLM as a prior in an RL training setup would actually work for new science moves in the same way. The important thing is to make sure that RL optimization is with goals in the world, not just [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback).  Basically, this is an exciting open area of research. Not something that we know can be solved now (in contrast to path one, which is available now).

<div class="section-divider"></div>

<div class="path-section pt-16">
<h6 class="path-label">PATH THREE</h6>
<h3 class="font-sans font-bold tracking-tight">AI as a Thinking Partner</h3>
</div>

The third path is somethign I think is more immediate, more appealing to scientists, and more likely to bear fruit in the short term. This is AI as a thinking tool. Scientists already do this. Additionally, it's totally orthogonal to any discussion about whether LLMs are "intelligent" or how to get AGI.

I use LLMs all the time to help me think through ideas. They're particularly helpful for math and code -- things that are hard to generate, but easy to verify. I've talked to PhD students in physics who claim ChatGPT (especially o3, but now 5) has completely changed their ability to solve tough problems. They even talk about learning to recognize “o3-shaped problems”, math problems that are particularly well-suited for ChagGPT o3. This is already a huge tool for thinking.

<div class="section-divider"></div>

### What’s Missing Right Now?

There are two scenarios that I think LLMs should be good at but are not:

<div class="grid-full">
  <ScenarioCards />
</div>

<!--
<div class="grid grid-cols-2">
<div class="">
> "I have a nail, find me a hammer"

I’ve identified a bespoke problem that I am a domain expert in. Search all the scientific literature to find some esoteric piece of math that solves the problem and explain it to me. This seems very doable right now. It's about building the right structures to facilitate it.

Models are okay at this, but nowhere near as good as they could be with only minor changes.
</div>
<div class="">
> "I have two hammers, can you combine them to build a screwdriver?"

Okay, this metaphor is not great, but it shows up a lot. Not only can I articulate the problem well, but I also have identified two relevant papers that provide partial solutions. If only I could combine them.
</div>
</div>
-->

Here, the breakthrough is in combining two existing ideas. Current LLMs are surprisingly bad at this. What they can do—which is suprising given that they can't direclty combine the two papers—is explain each paper well enough that I, the scientist, can then see the connection and do the combining myself. So the missing piece is the ability to actually do that synthesis.

Scientists naturally do this type of integration and synthesis. If we had all the training data in our heads, we couldn't help but see connections across fields and domains. Why can't the LLMs do it?

One explanation that seems likely to me is that the training data don't show the actual process of science. All of our scientific literature (all text books, papers, etc.) is written with the end product. In fact, we write stories that don't even resemble the process of discovery.

We don’t publish the dead ends, the bad hunches, the intuitions that didn’t pan out -- the **thinking**. We publish logical threads after we already know the answer. No wonder LLMs can't get there. Now, we could make breakthroughs in RL/continual learning and have AI-run labs that learn to think, but I don't think that's necessary. I think we can get there now with slightly better training data. And that training data likely already exists in the millions of chats between scientists and LLMs. Scientists are already thinking through their problems with LLMs. We just need better ways of flagging users and chats that highlight thinking and use those.

<div class="section-divider !mb-0"></div>

## The Opportunity Ahead

This is why I’m bullish about AI (and about the future more broadly). The debates about whether algorithms are “intelligent” or whether AGI is coming soon feel beside the point to this.

We don’t need AGI to make huge breakthroughs in science. I truly think we are just around the corner from having huge breakthroughs that result from LLMs. At this very moment, there are thousands of scientists talking to LLMs to help unlock a new way of thinking about their particular problems. At least one of them is going to make a breakthrough. Subtle finetuning of existing LLMs could dramatically increase the rate of these breakthroughs. This is in the absence of any progress on paths 1 and 2, which I find unlikely.

## Keeping Science Human
Zooming out, even if AI becomes better at certain aspects of science than the best scientsts, human scientists won't disappear. In the same way that people still play chess and Go. Although some elite players have retired and expressed that what made "Go" an appealing career is gone now, both go and chess are still played by millions of people and remains a career option. What becomes important is how we think about the role of human scientists. Historically, science has been about both *discovery* and *meaning* (i.e., making sense of the world). Although we will hand over more responsibility to machines for the discovery process, the latter will always be a human endeavor, and one that both governments and philanthropic organizations should already begin to put more emphasis on. Right now funding and prestige predominatnly reward new discoveries. With AI in science, we need to shift to preddominaently funding meaning making. We need to fund unifying theories, model interpretability, and cross-domain integration. Meta science and curation becomes increasingly important. What matters most right now is that governments and philanthropic organizations understand their role in creating a healthy scientific ecosystem: If machines discover facts, we must fund humans to discover meaning.

<div class="h-16"></div>


<div class="text-gray-500 text-base absolute -bottom-12">
Thanks to Rohan Pandey and Danielle Perszyk, with whom I've had conversations that helped crystallize several of these thoughts.
</div>
  </div>
</div>