---
title: How I learned to stop worrying and love the Free Energy Principle
date: 2025-8-19
description: A gentle introduction to the Free Energy Principle and the mother principle, KL Divergence minimization
draft: false
---

<script>
  import GaussianOptimizationDemo from "$src/components/GaussianOptimizationDemo.svelte";
    import { min } from "d3-array";
</script>

In my lab, we've been thinking a lot about how the brain learns a model of the world and how it uses it to act intelligently. This seems to be the crux of intelligence.

How do brains do this? If we understood this completely, we could build a brain. A brain like ours. That would be pretty cool. At that point, I'd have to close up shop and move on to some other research direction. Interestingly, out of all the theories in neuroscience, very few could ever be used to actually build a brain -- and most don't even pretend they could! However, there is one that at least claims to. It's called the Free Energy Principle. If you haven't heard of it, you should watch this nice [animated youtube short](https://www.youtube.com/watch?v=iPj9D9LgK2A) that introduces it quite well.

The [Free Energy Principle (FEP)](https://www.researchgate.net/profile/Mel-Andrews/publication/324246948_The_Free_Energy_Principle_An_Accessible_Introduction_to_its_Derivations_Applications_Implications/links/5cdb9a5f92851c4eaba05f25/The-Free-Energy-Principle-An-Accessible-Introduction-to-its-Derivations-Applications-Implications.pdf) is a so-called "general theory" and a lot of neuroscientists I know don't like it. There are two main types of criticisms I've heard: Either people can't make any sense of it, or it's "unfalsifiable" and they aren't sure if it is any [more useful than Bayesian inference](https://arxiv.org/pdf/1901.07945) or some more familiar theory.

This blog post is meant to be a gentle introduction to the math behind the FEP. We're going to start really simple and find that it stays pretty simple, but it gives us nice mathematical quantities for talking about concepts we are interested in. In the same way that Bayesian inference has been a powerful tool for formalizing certain perceptual phenomena, FEP provides a few additional quantities that are useful for talking about what brains do. Moreover, we're going to find that one quantity, the KL Divergence, is at the heart of it all. For a related (and genuinely more thoughtful) post, check out Hadi's blog [here](https://mysterioustune.com/2025/01/13/world-models-adaptation-and-surprise-part-1/#more-498). Finally, we're going to stop short of active inference here and punt entirely on action. I'll revisit that in a future blog post, but this will remain a good primer for that.

**Note:** The title here is a reference to a dark comedy from the height of the cold war called "Dr. Strangelove Or: how I learned to stop worrying and love the bomb". I haven't seen the movie since I was a kid, and I don't remember anything except the line **"not only is it possible. it is essential."** and that's how I now feel about the KL Divergence.

<img src="/kl/drfreeenergy.png" class="" style="max-width: min(100%, 38em)" alt="Left is the original movie poster. Right is the new and modified Dr. Free Energy" />
<p style="text-align: center; font-style: italic; margin-top: 0.5em; color: #666;">Left is the original movie poster. Right is the new and modified Dr. Free Energy, or: how I learned to stop worrying and love the Free Energy Principle. Karl Friston on top, and the "cybernetics seance" from the Macy conference (with Norbert Weiner, John von Neumann, Walter Pitts, Margaret Mead and others) on the bottom.</p>


## The problem
Let's start out with some solid ground: there IS a world out there. And the brain just gets samples from it through the senses. I'm going to massively over-simplify the problem in this blog post (don't worry, it still works even if we don't do this), but let's pretend the brain is passive and only receives sensory samples, $$X \sim p_{\text{world}}$$. 

$$p_{\text{world}}$$ is **the true generating processs** of the world for our passive brain. The brain does not have  access the true generating process, $$p_{\text{world}}$$, and so it cannot evaluate the density $$p_{\text{world}}(x)$$ even though it can get samples from it. For now, we're going to assume sammples are i.i.d. (ignoring actions/dynamics). That is a very incorrect assumption for brains, but it makes the math easier, so we're going to stick with it for now to gain some intuitions.

<img src="/kl/Pworld_Pbrain_simple.png" class="" style="max-width: min(100%, 22em)" alt="P world and P brain" />
<p style="text-align: center; font-style: italic; margin-top: 0.5em; color: #666;">The world generates data. The brain can only sample this data and must adjust its own internal model to match. In all cases, the brain can only evaluate its own model density p_brain.</p>

I'm going to be careful to keep track of what is in the world and what is in the brain, and will keep the notation clear. The brain can't evaluate $$p_{\text{world}}(x)$$, because that's literally the physics of the world, but it can evaluate its own model of it $$p_{\text{brain}}(x)$$ for any $$x \in X$$. Let's say the brain's model has parameters, $$\theta$$, so we write it as $$p_\theta(x)$$. How do you fit a good $$p_\theta(x)$$ to $$p_{\text{world}}(x)$$ given only samples? That's what we're going to solve here. 

What follows is a simple walkthrough in notation that I like, but is unusual in the active inference literature. I think you'll find that the math lends itself quite nicely to talking about what the brain is doing.

## Setup
This section lays out all the math facts that you need for all the derivations that come.

### **1) Log-likelihood**

Suppose we have the brain's model, $$p_\theta(x)$$, and i.i.d. samples from the world, $$x_1, \dots, x_n \sim p_{\text{world}}$$. The **likelihood** is just the probability density evaluated at the observed data combined across all samples:

$$
L(\theta) = \prod_{i=1}^n p_\theta(x_i).
$$

Given fixed data, the likelihood is a function of the parameters $$\theta$$. The reason we can multiply them all together is the i.i.d. assumption I made above. Because multiplying many small numbers quickly becomes impractical, we usually work with the **log-likelihood**, here the average log-likelihood or per-sample log-likelihood:

$$
\ell_n(\theta) = \frac{1}{n} \sum_{i=1}^n \log p_\theta(x_i).
$$

### Intuition
Think of the log-likelihood as a **surprise meter** with the sign flipped. If your model assigns high probability to what actually happened, the log-likelihood is high; if it assigns low probability, the log-likelihood is very negative. So if an improbable event happens, your negative log-likelihood spikes positive. Therefore, minimizing the negative log-likelihood, or equivalently, maximizing the log-likelihood, is just trying to reduce your surprise across many observations.


### **2) Expectations and the Law of Large Numbers**

An Expectation is the average quantity you would get from many draws under the data-generating law, $$p(x)$$. The expectation of a function $$f$$ is
$$
\mathbb{E}_{x\sim p}[f(x)] \;=\; \int f(x)\,p(x)\,dx 
\quad\text{(sum if \(x\) is discrete).}
$$

If $$x_1,\dots,x_n \stackrel{\text{iid}}{\sim} p$$, then by the law of large numbers
$$
\frac{1}{n}\sum_{i=1}^n f(x_i) \xrightarrow[]{\text{a.s.}} \mathbb{E}_{x\sim p}[f(x)].
$$

If we apply this to our per-sample log-likelihood above, we get:
$$
\ell_n(\theta) \;=\; \frac{1}{n}\sum_{i=1}^n \log p_\theta(X_i)
\;\xrightarrow[]{\text{a.s.}}\; \mathbb{E}_{x\sim p}\!\big[\log p_\theta(x)\big].
$$

*Intuition:* averaging samples from $$p$$ weights values by how often they occur. Points with larger $$p(x)$$ show up more and pull the average toward them, so the average of log-likelihood of the samples is a weighted average (weighted by $$p(x)$$) and converges to the expectation with enough samples.

*Importantly:* the expectation is w.r.t. $$p$$, the true generating process of the data. Our model log likelihood is evaluating $$p_\theta(x)$$, but it's evaluated at samples from $$p$$.


### **3) KL divergence**

The Kullback–Leibler (KL) divergence will emerge as a quantity of major interest. Here, I'm going to introduce it using it's definition, but I really want to emphasize that it emerges in the derivation below. I want to define it here so we're prepared to recognize it when it shows up, so we can interpret it accordingly. The KL divergence measures the mismatch between two distributions — in this case, the true world distribution $$P_{\text{world}}$$ and the brain’s model $$P_{\text{brain}}$$. Its definition is:

$$
D_{\mathrm{KL}}(P_{\text{world}}\|P_{\text{brain}})
= \mathbb{E}_{X \sim P_{\text{world}}}\!\left[ \log \frac{p_{\text{world}}(X)}{p_{\text{brain}}(X)} \right].
$$

### Intuition: KL as an expected log-likelihood ratio
Let's go back to basic statistics. For a **single sample** $$x$$, we know how to compare two models, $$P$$ and $$Q$$: we use the log-likelihood ratio: $$\log \frac{p(x)}{q(x)}$$. If we had many samples, we would average the log-likelihood ratio across all samples. With a large enough number of samples, the average converges to the expectation (that's just point 2 above).

Now, what happens if we're sampling from $$P$$ and then evaluating the log-likelihood ratio compared to $$Q$$? Well, then we get the KL divergence! It's an expected log-likelihood ratio when you're sampling from the first density. It's bounded at $$0$$. It has to be positive. And what it tell us is how much extra “surprise” we get when we use $$Q$$ (the wrong model) to explain data generated by $$P$$. 

For our problem, if the brain’s internal model matches the world perfectly, the KL is 0 — no extra surprise. But the more the brain’s predictions diverge from the world’s samples, the larger the KL becomes. In other words, KL measures the cost of pretending the brain’s model generated the data when in fact it came from the world. This is exactly what we want to minimize. Small KL is the mathematical equivelent of saying the "the brain's model fits the world well".


### Properties worth knowing
- $$D_{\mathrm{KL}}(P \| Q) \geq 0$$, and it’s 0 iff $$P=Q$$.  
- It’s **asymmetric**: $$D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P)$$.  
- Connection to cross-entropy:  
  $$
  D_{\mathrm{KL}}(P\|p_\theta) = H(P, p_\theta) - H(P),
  $$  
  where $$H(P, p_\theta) = \mathbb{E}_{P}[-\log p_\theta(X)]$$ is the cross-entropy.

## Maximum likelihood is KL minimization

Given fixed data, the log-likelihood is a function of the model parameters $$\theta$$, and all it tells you is the log probability of each sample. If we maximize the log-likelihood (or equivalently, minimize the negative log-likelihood), what happens? This is a classic technique in statistics called maximum likelihood estimation, and we're going to walk through a visual example of what it looks like when the average log-likelihood is maximized.

Below is a simple example with a true generating distribution that is a 2D Gaussian. The data, $$x_i$$, are shown in red. Our model is also a 2D Gaussian and is shown in blue. The model, $$p_\theta(x) = \mathcal{N}(\mu, \Sigma)$$, with parameters $$\theta = (\mu, \Sigma)$$. Withough knowing the true parameters, we can evaluate the average log-likelihood of the data under our model, and we can adjust our parameters to maximize this quantity by stepping along the derivative with respect to the parameters. At first, our model is not overlapping with the data. You can hit "Run Optimization" to see what happens if we simply step along the gradient (derivative) of the log-likelihood.

<GaussianOptimizationDemo />
  
What happened? Well, our parameters converged to the true parameters that generated the data. Try hitting reset and running it a few times. The blue density (our model) starts in a random location and orientation, but it always converges to the true distribution. You can also see the parameters converge. I've listed the true mean and the model estimate. 


It's fun to watch the animation, and is a totally standard method in statistics. But did you ever stop to ask why this works? Why does assigning high probability to likely samples make our model fit the true generating distribution?

What is happening when we maximize likelihood? Or, equivalently, when we minimize negative log-likelihood?

Let's rearrange some terms using the fun facts from above and see what happens. With enough samples, we can use **point 2** above. With a large number of samples, the average negative log-likelihood converges to the **expected negative log-likelihood** (NLL) under the true world distribution:

$$
\mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_\theta(x)\big].
$$

Make it equal to itself and add and subtract $$\mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_{\text{world}}(x)\big]$$:

$$
\begin{aligned}
\mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_\theta(x)\big]
&= \mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_\theta(x)\big]  + \mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_{\text{world}}(x)\big] - \mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_{\text{world}}(x)\big]  \\
\end{aligned}
$$

Now we can combine the first two terms using the log rule $$\log a - \log b = \log \frac{a}{b}$$:

$$
\begin{aligned}
&= \mathbb{E}_{x\sim p_{\text{world}}}\!\left[-\log \frac{p_\theta(x)}{p_{\text{world}}(x)} - \log p_{\text{world}}(x)\right] \\
\end{aligned}
$$
And then split the expectation and simply recognize the KL divergence and the entropy of the world:
$$
\begin{aligned}
&= \underbrace{\mathbb{E}_{x\sim p_{\text{world}}}\big[-\log p_{\text{world}}(x)\big]}_{\text{world entropy } H(p_{\text{world}})} 
+ \underbrace{\mathbb{E}_{x\sim p_{\text{world}}}\!\left[\log \frac{p_{\text{world}}(x)}{p_\theta(x)}\right]}_{D_{\mathrm{KL}}(p_{\text{world}}\|p_\theta)}.
\end{aligned}
$$

So there you have it. We started with the average log-likelihood and rerranged the terms and it revealed that minimizing the negative log-likelihood is equivalent to minimizing the KL divergence between the world and the brain's model.

$$
\underbrace{\mathbb{E}_{X\sim P_{\text{world}}}\big[-\log p_\theta(X)\big]}_{\text{expected NLL}}
\;=\;
\underbrace{H\!\big(P_{\text{world}}\big)}_{\text{world entropy (constant in }\theta\text{)}}
\;+\;
\underbrace{D_{\mathrm{KL}}\!\big(P_{\text{world}}\;\|\;p_\theta\big)}_{\text{model mismatch (world }\|\text{ brain)}}.
$$

The first term (entropy of the world) does not depend on $$\theta$$. Therefore, minimizing expected NLL **is the same as** minimizing the KL divergence between $$P_{\text{world}}$$ and $$P_{\text{brain}}$$! 

That simple result is satisfying: Maximizing likelihood makes the brain’s model assign high probability to what the world actually produces AND it tunes $$P_{\text{brain}}$$ to get as close as possible to $$P_{\text{world}}$$. This intuition helps unpack Alex Alemi's wonderful [blog post](https://blog.alexalemi.com/kl-is-all-you-need.html) on KL divergences and how central they are. Maximum likelihood is just a special case of KL minimization. Additionally, this is not true for any arbitrary distance metric or divergence. As far as I know, the KL divergence has a priveleged status.



<img src="/kl/MLvsKL.png" class="" style="max-width: min(100%, 38em)" alt="KL is better" />

## Hidden causes: the brain’s internal variables

So far we treated the brain’s model as a direct mapping from observations to probabilities, $$p_\theta(x)=p_{\text{brain}}(x)$$. That’s too simple, because the brain needs to be able to flexibly encode the state of the world in terms of *hidden causes*. Let's call these hidden causes $$Z$$. Importantly, these are not the “real” physical causes; they’re useful internal variables the brain uses to understand the world.

<img src="/kl/Pworld_Pbrain_bayesian.png" class="" style="max-width: min(100%, 22em)" alt="P world and P brain (Bayesian)" />
<p style="text-align: center; font-style: italic; margin-top: 0.5em; color: #666;">All samples of data come from the true generating process p_world. The brain wants to explain these samples using a generative model with hidden causes, z. The brain can only sample from p_world, but it does not evaluate it. To fit the data well, it must adjust the parameters of its internal model as well as infer the latent causes. Importantly all these causes are in the brain NOT the world. The density that generates samples is the world. The densities used for inference are in the brain.</p>

The brain’s model says: first sample a hidden cause $$Z$$ from a prior, then generate an observation $$X$$ from a likelihood.

$$
p_{\text{brain}}(x,z) \;=\; p_\theta(x) \;=\; \int p_\theta(x,z)\,dz.
$$

Here we have the static parameters of the brain $$\theta$$, which could map onto the weights of a neural network (or the synapses in a brain). And we have the latent variables $$Z$$, which are the internal latent variables the brain uses to understand the world (which could map onto the activations or spikes of neurons).

Two things we want to do with this model:

1. **Learning** (adapt the world's statistics): make $$p_{\text{brain}}(x)$$ match $$P_{\text{world}}(x)$$ as well as possible using samples $$X\sim P_{\text{world}}$$.
2. **Inference** (adapt to the sampled data): given an observation $$x$$, infer its hidden causes via the *posterior*
   $$
   p_{\text{brain}}(z\mid x) \;=\; \frac{p_{\text{brain}}(x\mid z)\,p_{\text{brain}}(z)}{p_{\text{brain}}(x)}.
   $$

This is just Bayes rule and it maps nicely on to [words we use to describe perception](https://jake.vision/blog/inside-out-perspective). But, I want to spend a moment to belabor a recurring issue in perceptual psychology.The way Bayesian inference is typically introduced in perception is that the brain is inferring the causes in "the world" from "the senses".

<img src="/kl/bayes_rule.png" class="" style="max-width: min(100%, 48em)" alt="Bayes Rule" />

In a typical Bayesian Brain experiment, psychologists and neuroscientists will test to see if the subject has behavior that looks like Bayesian inference *over the parameters of their experiment*. For example, they might show drifting motion where some motions are more likely than others and see if the subjectls learn to integrate the prior probabilities of the motion (in the experiment) with the incoming visual evidence.

But that's NOT what is happening here. $$Z$$ are in $$P_{\text{brain}}$$. They are NOT in $$P_{\text{world}}$$. The experiment is part of $$P_{\text{world}}$$, but $$Z$$ are not. To learn to act intelligently in the world, $$Z$$ likely have high mutual information between relevant groundtruth causes in the world -- the intuitive physics level, but no more. The key point is to remember that $$Z$$ are not the "real" causes in the world (or your experiment). They are the brain's internal representation of the world.

<img src="/kl/bayes_rule_brain.png" class="" style="max-width: min(100%, 48em)" alt="Bayes Rule for P Brain" />

Now, even though $$Z$$ are just causes that the brain made up, that denominator $$p_{\text{brain}}(x)=\int p_{\text{brain}}(x,z)\,dz$$ is still usually intractable, which makes the exact posterior $$p_{\text{brain}}(z\mid x)$$ intractable too. 

We're going to get around this by inventing a density we *can* evaluate and just try to optimize the parameters for that. We just invent a density we can evaluate, $$q_\phi(z\mid x)$$. Is this even reasonable? In the next section we'll see that it is and why it works is quite satisfying.

<img src="/kl/Pworld_Pbrain_variational.png" class="" style="max-width: min(100%, 22em)" alt="P world and P brain (Variational)" />
<p style="text-align: center; font-style: italic; margin-top: 0.5em; color: #666;">The world still generates data. All samples come from p_world. The brain wants to learn the hidden causes in its generative model of the data. Again, it can only sample from p_world, but it does not evaluate it and z all live in p_brain. The brain adjusts its parameters, theta, and infers the latent causes, z. To do this, it uses a variational approximation to the true posterior. This is a distribution it knows how to evaluate and avoides the intractable integral.</p>

## Deriving the Evidence Lower Bound (a.k.a. Free Energy)

To get around the intractable posterior, we just invent a density we can evaluate, $$q_\phi(z\mid x)$$. This is the backbone of variational inference, and what we're going to do here is derive a quantity known in machine learning as the **Evidence Lower Bound** (ELBO). The ELBO is typically derived using something called Jensen's inequality, so I'll show that first, but then we'll do it without it to see what we were missing.

### With Jensen’s inequality (lower bound)


** Jensen’s inequality**
Jensen's inequality says that the average of a logarithm is always less than (or equal to) the logarithm of the average:
$$
\log \mathbb{E}[Y] \;\ge\; \mathbb{E}[\log Y]
$$
It's often explained in terms related to concavity, but if you think about the shape of the logarithm, it's pretty intuitive. The logarithm is *compressive* for large values and *explosive* near zero, so a few tiny $$Y$$ values pull the *average of logs* way down. If you *average first*, those tiny values are cushioned before taking the log.

Here's the derivation you see in most places:
Start from the model evidence:
$$
\log p_\theta(x)
= \log \int p_\theta(x,z)\,dz
= \log \int q_\phi(z\mid x)\,\frac{p_\theta(x,z)}{q_\phi(z\mid x)}\,dz.
$$

Apply Jensen’s inequality (log of an expectation ≥ expectation of the log):
$$
\log p_\theta(x)
\;\ge\;
\mathbb{E}_{q_\phi}\!\left[\log \frac{p_\theta(x,z)}{q_\phi(z\mid x)}\right]
$$

And we're done! Call that thing the **ELBO**:
$$
\mathrm{ELBO}(x;\theta,\phi)
\;:=\;
\mathbb{E}_{q_\phi}[\log p_\theta(x,z)] \;-\; \mathbb{E}_{q_\phi}[\log q_\phi(z\mid x)]
\;=\;
\mathbb{E}_{q_\phi}[\log p_\theta(x\mid z)] \;-\; D_{\mathrm{KL}}\!\big(q_\phi(z\mid x)\,\|\,p_\theta(z)\big).
$$

Importantly, the **(variational) Free Energy** is $$\mathcal{F}(x;\theta,\phi):=-\mathrm{ELBO}(x;\theta,\phi)$$:
$$
\mathcal{F}(x;\theta,\phi)
\;=\;
\mathbb{E}_{q_\phi}\!\Big[\log q_\phi(z\mid x) - \log p_\theta(x,z)\Big].
$$

There you go! It's not that mystical how to get there. **But what is it good for?**

Well, we can see from the inequality that it's a bound on the "model evidence"... what we were calling log-likelihood at the top, $$p_\theta(x)$$

But **what disappeared in that inequality?** What are we actually doing when we minimize Free Energy?

Let's rederive without Jensen and see what we're missing.

### Without Jensen (the exact identity)

Start with any tractable density $$q_\phi(z\mid x)$$ (whose integral is 1):

$$
\begin{aligned}
\log p_\theta(x)
&= \log p_\theta(x)\!\int q_\phi(z\mid x)\,dz  \qquad\quad\ \ \text{(insert 1 = }\int q_\phi) \\
&= \int q_\phi(z\mid x)\,\log p_\theta(x)\,dz \qquad\qquad\text{(move constant inside)}\\
&= \mathbb{E}_{z\sim q_\phi(z\mid x)}\!\big[\log p_\theta(x)\big] \\
&= \mathbb{E}_{q_\phi}\!\Big[\log p_\theta(x) + \log p_\theta(z\mid x) - \log p_\theta(z\mid x)\Big] \quad \text{(add and subtract)}\\
&= \mathbb{E}_{q_\phi}\!\Big[\log \tfrac{p_\theta(x)\,p_\theta(z\mid x)}{p_\theta(z\mid x)}\Big] \\
&= \mathbb{E}_{q_\phi}\!\Big[\log \tfrac{p_\theta(x,z)}{p_\theta(z\mid x)}\Big] \qquad\qquad\ \ \ \ \ \text{(Bayes: }p_\theta(x)\,p_\theta(z\mid x)=p_\theta(x,z))\\
&= \mathbb{E}_{q_\phi}\!\Big[\log \tfrac{p_\theta(x,z)\,q_\phi(z\mid x)}{p_\theta(z\mid x)\,q_\phi(z\mid x)}\Big] \quad \text{(multiply/divide by }q_\phi)\\
&= \mathbb{E}_{q_\phi}\!\Big[\log \tfrac{p_\theta(x,z)}{q_\phi(z\mid x)}\Big]
   \;+\; \mathbb{E}_{q_\phi}\!\Big[\log \tfrac{q_\phi(z\mid x)}{p_\theta(z\mid x)}\Big] \\
&= \underbrace{\mathbb{E}_{q_\phi}\!\Big[\log \tfrac{p_\theta(x,z)}{q_\phi(z\mid x)}\Big]}_{\displaystyle \text{ELBO}(x;\theta,\phi)}
   \;+\;
   \underbrace{D_{\mathrm{KL}}\!\big(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\big)}_{\text{inference gap}\ \ge 0}.
\end{aligned}
$$

<div class="w-full text-center">
   Therefore,
   </div>

$$
\boxed{\ \log p_\theta(x) \;=\; \mathrm{ELBO}(x;\theta,\phi) \;+\; D_{\mathrm{KL}}\!\big(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\big)\ }.
$$

### What does this mean? 
Well, now we can see clearly what disappeared in the Jensen's inequality derivation: the KL divergence between the approximate posterior and the true posterior. This is not a bound anymore. It's an exact identity. Now, because the KL is always $$\ge 0$$, if we maximize ELBO using the varational posterior parameters $$\phi$$, we are guaranteed to minimize the KL divergence between the approximate posterior and the true posterior. If we maximize ELBO using the model parameters $$\theta$$, we can push up the model evidence and mimimize the KL between the world and the brain's model. Thus, maximizing ELBO or minimizing Free Energy minimizing is particularly useful during inference for minimizing the KL between the variational posterior and the true posterior.


### Putting it all together: What about learning? How do we make the brain's model match the world?

So far we showed that maximum likelihood can be interpreted as minimizing the KL divergence between the world and the brain's model. We also showed that minimizing Free Energy is equivalent to minimizing the KL divergence between the variational posterior and the true posterior. Now we're going to combine them both to see what minimizing Free Energy is really doing.

First, remember that Free Energy is the negative ELBO:
$$
\mathcal{F}(x;\theta,\phi)
\;=\;
-\mathrm{ELBO}(x;\theta,\phi).
$$

<div class="w-full text-center">
so, 
</div>

$$
\mathcal{F}(x;\theta,\phi)
\;=\;
-\log p_\theta(x) \;+\; D_{\mathrm{KL}}\!\big(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\big).
$$

Second, remember our trick up above that minimizing the negative log-likelihood is the same as minimizing the KL divergence between the world and the brain's model:

$$
\underbrace{\mathbb{E}_{X\sim P_{\text{world}}}\big[-\log p_\theta(X)\big]}_{\text{expected NLL}}
\;=\;
\underbrace{H\!\big(P_{\text{world}}\big)}_{\text{world entropy (constant in }\theta\text{)}}
\;+\;
\underbrace{D_{\mathrm{KL}}\!\big(P_{\text{world}}\;\|\;p_\theta\big)}_{\text{model mismatch (world }\|\text{ brain)}}.
$$

Let's put these together and see what happens when we minimize the Free Energy:

$$
\boxed{
\;\mathbb{E}_{X\sim P_{\text{world}}}\big[\mathcal{F}(X;\theta,\phi)\big]
=
\underbrace{H\!\big(P_{\text{world}}\big)}_{\text{constant in }\theta,\phi}
+
\underbrace{D_{\mathrm{KL}}\!\big(P_{\text{world}}\,\|\,p_\theta\big)}_{\text{model mismatch (world }\|\text{ brain)}}
+
\underbrace{\mathbb{E}_{X\sim P_{\text{world}}}\!\left[D_{\mathrm{KL}}\!\big(q_\phi(z\mid X)\,\|\,p_\theta(z\mid X)\big)\right]}_{\text{inference mismatch }(q\|\text{ true brain posterior})}
\;}
$$

**Conclusion (what minimizing Free Energy does):**
- Improves the **generative model** (minimizes $$D_{\mathrm{KL}}(p_{\text{world}}\|p_\theta)$$).  
- Improves **inference** (minimizes the expected $$D_{\mathrm{KL}}\big(q_\phi(z\mid X)\,\|\,p_\theta(z\mid X)\big)$$).  
- The world’s entropy is constant in $$\theta$$ and $$\phi$$ — you can’t change physics; you can only make your **brain model** and **inference** better.


## Conclusion
In this blog post, we derived the free energy principle from first principles. We learned that simply trying to assign high probability to probable events is equivalent to making the brain's model fit the world well (by minimizing KL divergence). We learned that the Evidence Lower Bound (ELBO) is pretty easy to derive, even without Jensen's inequality and that it leads to an exact identity rather than a bound, where maximizing ELBO is actually minimizing two intractable KLs that we're really interested in minimizing. The KL Divergence emerges as a metric of how good our models are in two places. The KL between the world and the brain's model tells us how well the brain's model fits the world (Learning). The KL between the variational posterior and the true posterior tells us how well the brain's inference matches the true posterior (Inference). Minimizing Free Energy is equivalent to minimizing both of these KLS! And the KL is really the mother principle!



<img src="/kl/klmeme.png" class="" style="max-width: min(100%, 38em)" alt="Everything is KL minimization" />
