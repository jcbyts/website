

OLD TEXT HERE

## Geometry
Geometry isn’t really about shapes. It is about how things relate to each other and the preservation of those relationships under certain transformations.

- Rotate a square, and it’s still a square.

- Transpose a melody into a higher key, and it’s still the same tune.

- Move an object and all its parts move in relation to eachother.

This is my working definition of geometry: the relational structure of things and the transformations that preserve that structure. Both the relationships and the transformations should be learned from data, but the architecture should make that learning natural rather than fragile.

This idea shows up everywhere. Many ancient cultures discovered geometry independently—Euclid in Greece, Liu Hui in China, the Sulba Sutras in India. They also discovered music scales built on frequency ratios. Why? Because these are universal truths: the world is structured around such invariances, and brains are especially good at discovering them.

A striking demonstration comes from William Molyneux’s question: could a person born blind, upon gaining sight, recognize shapes they had only known by touch? Does the learned geometry of touch generalize to vision? Amazingly, there are empirical answers to this now. People who were blind from birth but recently gained sight (through cataract surgery) were able to learn to link visual experience to objects they had previously only experienced through touch in very little [experience](https://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Molyneux_NatureNeuro2011.pdf). Similarly, chickens [solve Molnyneux's problem](https://royalsocietypublishing.org/doi/10.1098/rsbl.2024.0025). Geometry, then, becomes a Rosetta stone for the different parts of the brain. It is the common representational format to translate across modalities.

Now, I am not claiming that geometry is explicitly hard coded, but whatever architecture is used to build intelligence must learn representations that preserve relations when inputs are transformed. The technical term here is [equivariance](https://en.wikipedia.org/wiki/Equivariant_map). In the extreme, this might be explicitely represented: like in the way the convolutions in a CNN are equivariant to [translations](https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59). This is the pursuit of [geometric deep learning](https://thegradient.pub/towards-geometric-deep-learning/). It is possible that geometric deep learning will prove the most fruitful, but I suspect something more relaxed will work as well. What is most important is that moving features around in latent space should preserve the relationships between them just as moving things around in the world should preserve the relationships between them.

LLMs don’t really capture geometry like I mean here. We do see linear substructure and feature subspaces (e.g., “king–man+woman≈queen”-style structure generalized to transformers). And, of course, all the training data has relational structure, but many features live in superposition—entangled in shared dimensions rather than cleanly factorized. That makes transformations brittle and context-dependent rather than preserved. In neuroscience, this can be [a good thing](https://www.nature.com/articles/nature12160), but it is brittle. This is also why some researchers call LLMs paradoxically under-parameterized. Not because they’re too small — they have billions of weights — but because the way they represent concepts doesn’t carve out stable dimensions for preserving relationships. They are underparameterized because they have to represent every instance of a concept, rather than the concept and transformations of it. 

At their worst, LLMs suffer just like Borges's "Funes the Memorious". In "Funes the Memorious", The titular character has a sort of brain damage where he remembers everything in excruciatingly particular detail but is unable to grasp abstract ideas. A dog seen at noon and later in the day are two different things to Funes. If this is your reality, with no ability to represent transformations, you can't have intelligence. LLMs are not this bad, but it's important to highlight my use of the word "geometry". It is about architectures that preserve relationships under (certain) transformations. This will be the glue that holds the other two ingredients together. There have been attempts to build structure into attention mechanism (e.g., [slot attention](https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html)), but I believe this is not used in LLMs, which rely mostly on the vanillia attention mechanism and massive amounts of data.

## Hierarchy

Intelligence isn’t about recording detail. In fact, it's generally the opposite, it's about throwing away detail, abstracting. The way we solve this is with hierarchy: compressing fine-grained signals into coarser, more abstract ones. Hierarchical abstraction is essential for reasoning. When deciding which college to attend or job to take, you don’t simulate every sock you’ll wear. You think in broader abstractions. Similarly, in visual cortex, neurons at early stages respond to edges; higher stages combine those into shapes, then objects, then categories. 

Now wait a minute. Don't deep neural networks and LLMs already do this too? Isn't that what the "deep" in deep learning is all about? Yes, but not exactly the way I mean it. There are two ways to understand what I mean by hierarchy. First is from an inference point of view: Although the deeper layers in deep neural networks do get more abstract (e.g., object selectivity emerges in deep layers of CNNs), these models don't reach back into the lower levels and modify them once they've settled on a high level concept. In contrast, [hierarchical inference](https://pubmed.ncbi.nlm.nih.gov/12868647/)does exactly that. Once an abstract interpretation is settled on, the low level features that explain are reinterpreted to be consistent with it. This is an iterative process called "explaining away" and we think is true of brains and perception, but it has been difficult to scale on current hardware.

The second way to understand the difference is about how representations connect across layers. Large LLMs have [emergent](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) geometry and hierarchy, but because concepts are entangled within layers, connections are brittle between them. This means that if I move a high level concept around, the low-level details don't move necessarily move with it. Think about zooming out on Google maps, the streetnames and details disappear and more abstract concepts like pedestrian zones become visible. That is similar in the layers of a CNN or transformer. But now move the map and zoom back in and all the details are still there. That is hard with transformers or CNNs. For example, if I move a coffee cup from the table to my mouth, the image drawn on the side of the cup will move with it. Video models struggle with ths scenario. Sora 2 for example, promises ["stronger frame consistency"](https://dev.to/alifar/sora-2-next-generation-text-to-video-ai-explained-acl). Why do they have problems with frame consistency in the first place? Because details are only losely connected to the abstract high level concepts.
 
A true hierchical representation is one where moving the low-level details affects the high-level concepts *and vice versa*!. This bidirectional consistency is essential.  And this is exactly where geometry and hierarchy intertwine: emergent hierarchy and geometry doen’t hold together as well as if the architecture natively supports both.

## Action

Action is the most important, and it has gotten a lot of attention recently as we've moved into the agentic and [robotics directions](https://sergeylevine.substack.com/p/sporks-of-agi). Unlike next-token-prediction, we act to learn: move our eyes, ask questions, try an experiment. In fact, learning largely doesn't happen without [action](https://dl.acm.org/doi/10.1145/3325480.3325525). This is a point that Sutton [makes](https://www.dwarkesh.com/p/richard-sutton) and I agree with him on this.

To motivate this, I want to draw an example from our recent AI past: [Move 37](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol). Move 37 was a move played by AlphaGo in its historic match against Lee Sedol in 2016. Move 37 was a move that no human would make (I don't know enough about Go to say more) and AlphaGo ended up winning the game. I think it's fairly safe to argue that a model trained only using next-token prediction on human games likely wouldn’t invent it. Action mattered. This is a fairly simple example someone could test (does next token prediction on human Go moves ever discover new moves?).

LLM-based digital agents now all have some RL on top of it, but it's probably not enough. They are still burdened with tHe represnetations they learned during pretraining.  They are not good representations for action. As already mentioned in the above sections (and detailed thoroughly by Anthropic) they are, in fact, particularly bad representations for acting. They are entangled. 

### Good representations are actionable

A real brain is not given data. It's housed inside a body and all sense data from the world is captured while moving through it. Hold your self as still as possible and you will notice that it is impossible. Your eyes move from word to word as you read this. So, the first thing you need to learn in the world is how to disentangle the self from all the input data. 

That is essential for intelligence. To be able to learn a world model and simulate counterfactual universes from it, you need to be able to disentangle the self from the world. Further, you ened to be able to infer what in the world will change when you take an action. This is essentialy what Gibson meant when he said we perceive affordances.


Explicit hierarhcy naturally leads to [more disentangled representations](https://proceedings.neurips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html) when the data are truly hierarchical. There was a brief moment where the field cared a lot about disentanglement, but this was largely abandoned in favor of [untangling](https://pubmed.ncbi.nlm.nih.gov/17631409/) (i.e., finding linearly decodable concepts) [after training](https://transformer-circuits.pub/2022/toy_model/index.html#strategic-ways-out) rather than inducing disentanged representations (training models where individual dimensions interpretable). In general, it is very unlikely to get disentangled representations [without strong inductive biases](https://arxiv.org/abs/1811.12359).


This becomes a very important concept when you are an agent. Whether physical or digital, once you have action, signatures of your own actions are entangled in the data. To learn a good world model, you need to be able to disentangle the self from the world. This is essential for intelligence.

An action-conditioned state transition model is the essential building block for intelligence. In this sence the Friston camp is right. The [``universal generative model''](https://www.verses.ai/research-blog/why-learn-if-you-can-infer-active-inference-for-robot-planning-control) has this key ingredient. And often, it comes with hierarchy. Where it fails, is that the models are often linear generative models and these representations do not facilitate the learning of transformations or the preservation of relationships across levels or with action. And that is essential.

