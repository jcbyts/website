---
title: Recipe for Intelligence
date: 2025-9-30
description: A recipe for building representations that support intelligence
draft: true
---

## AGI or Stone Soup?

There's no denying that Large language models (LLMs) have been absolutely astonishing. Despite this, academics have largely denied that there is any intelligence there. Gary Marcus is the [loudest](https://fortune.com/2025/08/24/is-ai-a-bubble-market-crash-gary-marcus-openai-gpt5/) of the bunch, but its a fairly common stance: *whatever this is, it's not intelligence*. My favorite version of the critique is from Alison Gopnik, where she borrows the folktale of [Stone Soup](https://simons.berkeley.edu/news/stone-soup-ai) as a metaphor. The story involves a group of travelers who have no food and trick the villagers into providing all the ingredients for a delicious soup by saying they are making "stone soup", but of course it goes better with all the real food ingredients the villagers provide. The moral here is that LLMs only appear to be intelligent because they got the end product of our intelligence as the ingredients. They provided the stones and the cauldron and we poured in all the intelligence.

What is the right recpie for intelligence? What ingredients do we need? In contrast to the academics, experts on the industry side claim AGI "takeoff" might happen as soon as [2027](https://ai-2027.com/) and there is widespread faith in [the Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). But now even Richard Sutton -- Mr. Bitter Lesson himself -- has joined the naysayers insiting that LLMs [can't get there](https://garymarcus.substack.com/p/game-over-for-pure-llms-even-turing). His key ingredient is action, and it's one of my list as well. 

No matter how you slice it, we've run out of data, next-token prediction plus RL is not quite there. I think there are three key ingredients that facilitate the learning of representations that support intelligence. I think any system that has these ingredients will be capable of intelligence. I think without all three, we won't get there.

## The Ingredients

In this post, I'm going to focus on the ingredients, because there are probably many ways to combine them. I think all intelligence will have these three core features. These are necessary (and possibly sufficient) for represnetations that support intelligence.  I introduce these in a particular order because they build on each other in sequence:

-**Geometry** (Representation of Transformations)

-**Hierarchy** (Coarse Graining)

-**Action** (Active inference)

Of course Hierarchy is familiar to anyone who has worked with deep learning. And action is familiar to anyone in RL. I'm going to make these definitions more precise below and suggest ways in which these things interact. Importantly, LLMs already have aspects of these ingredients, but they need all three.

## Geometry
Geometry isn’t really about shapes. It is about how things relate to each other and the preservation of those relationships under certain transformations.

- Rotate a square, and it’s still a square.

- Transpose a melody into a higher key, and it’s still the same tune.

- Move an object and all its parts move in relation to eachother.

This is my working definition of geometry: the relational structure of things and the transformations that preserve that structure.

This is *central* to understanding the world. And it is why geometry is the closest thing we have to a universal language. Ancient cultures discovered geometry independently: Greece (Euclid), China (Liu Hui), India (Sulba Sutras). Similarly, they discovered music theory and scales as frequency ratios. Why? Because it is a universal description of spacetime in the world we experience. 

These concepts so ubiquitious it seems possible we have geometric schemas we use for thought. In the 1600s, William Molyneux posed a question about whether a blind person who suddenly gains sight would be able to match a visual stimulus to a familiar tactile one. Does the learned geometry of touch generalize to vision? Amazingly, there are empirical answers to this now. People who were blind from birth but recently gained sight (through cataract surgery) were able to learn to link visual experience to objects they had previously only experienced through touch in very little [experience](https://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Molyneux_NatureNeuro2011.pdf). Similarly, chickens [solve Molnyneux's problem](https://royalsocietypublishing.org/doi/10.1098/rsbl.2024.0025). Geometry, then, becomes a Rosetta stone for the different parts of the brain. Vision, touch, and movement don’t share raw signals, but they can all agree on relational structure.

Now, I am not claiming that geometry is explicitly hard coded, but whatever architecture is used to train intelligence must facilitate the learning of transformations. Technically, this is called [equivariance](https://en.wikipedia.org/wiki/Equivariant_map). In the extreme, this might be explicitely represented: like in the way the convolutions in a CNN are equivariant to [translations](https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59). This is the pursuit of [geometric deep learning](https://thegradient.pub/towards-geometric-deep-learning/). It is possible that geometric deep learning will prove the most fruitful, but I suspect something more relaxed will work as well. What is most important is that moving features around in latent space should preserve the relationships between them just as moving things around in the world should preserve the relationships between them.

LLMs don’t really capture geometry like I mean here. We do see linear substructure and feature subspaces (e.g., “king–man+woman≈queen”-style structure generalized to transformers). And, of course, all the training data has relational structure, but many features live in superposition—entangled in shared dimensions rather than cleanly factorized. That makes transformations brittle and context-dependent rather than preserved. In neuroscience, this can be [a good thing](https://www.nature.com/articles/nature12160), but it is brittle. This is also why some researchers call LLMs paradoxically under-parameterized. Not because they’re too small — they have billions of weights — but because the way they represent concepts doesn’t carve out stable dimensions for preserving relationships. They are underparameterized because they have to represent every instance of a concept, rather than the concept and transformations of it. 

At their worst, LLMs suffer just like Borges's "Funes the Memorious". In "Funes the Memorious", The titular character has a sort of brain damage where he remembers everything in excruciatingly particular detail but is unable to grasp abstract ideas. A dog seen at noon and later in the day are two different things to Funes. If this is your reality, with no ability to represent transformations, you can't have intelligence. LLMs are not this bad, but it's important to highlight my use of the word "geometry". It is about architectures that preserve relationships under (certain) transformations. This will be the glue that holds the other two ingredients together. There have been attempts to build structure into attention mechanism (e.g., [slot attention](https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html)), but I believe this is not used in LLMs, which rely mostly on the vanillia attention mechanism and massive amounts of data.

## Hierarchy

Intelligence isn’t about recording detail. In fact, it's generally the opposite, it's about throwing away detail, abstracting. The way we solve this is with hierarchy: compressing fine-grained signals into coarser, more abstract ones. Hierarchical abstraction is essential for reasoning. When deciding which college to attend or job to take, you don’t simulate every sock you’ll wear. You think in broader abstractions. Similarly, in visual cortex, neurons at early stages respond to edges; higher stages combine those into shapes, then objects, then categories. 

Now wait a minute. Don't deep neural networks and LLMs already do this too? Isn't that what the "deep" in deep learning is all about? Yes, but not exactly the way I mean it. There are two ways to understand what I mean by hierarchy. First is from an inference point of view: Although the deeper layers in deep neural networks do get more abstract (e.g., object selectivity emerges in deep layers of CNNs), these models don't reach back into the lower levels and modify them once they've settled on a high level concept. In contrast, [hierarchical inference](https://pubmed.ncbi.nlm.nih.gov/12868647/)does exactly that. Once an abstract interpretation is settled on, the low level features that explain are reinterpreted to be consistent with it. This is an iterative process called "explaining away" and we think is true of brains and perception, but it has been difficult to scale on current hardware.

The second way to understand the difference is about how representations connect across layers. Large LLMs have [emergent](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) geometry and hierarchy, but because concepts are entangled within layers, connections are brittle between them. This means that if I move a high level concept around, the low-level details don't move necessarily move with it. Think about zooming out on Google maps, the streetnames and details disappear and more abstract concepts like pedestrian zones become visible. That is similar in the layers of a CNN or transformer. But now move the map and zoom back in and all the details are still there. That is hard with transformers or CNNs. For example, if I move a coffee cup from the table to my mouth, the image drawn on the side of the cup will move with it. Video models struggle with ths scenario. Sora 2 for example, promises ["stronger frame consistency"](https://dev.to/alifar/sora-2-next-generation-text-to-video-ai-explained-acl). Why do they have problems with frame consistency in the first place? Because details are only losely connected to the abstract high level concepts.
 
A true hierchical representation is one where moving the low-level details affects the high-level concepts *and vice versa*!. This bidirectional consistency is essential.  And this is exactly where geometry and hierarchy intertwine: emergent hierarchy and geometry doen’t hold together as well as if the architecture natively supports both.

## Action

Action is the most important, and it has gotten a lot of attention recently as we've moved into the agentic and [robotics directions](https://sergeylevine.substack.com/p/sporks-of-agi). Unlike next-token-prediction, we act to learn: move our eyes, ask questions, try an experiment. In fact, learning largely doesn't happen without [action](https://dl.acm.org/doi/10.1145/3325480.3325525). This is a point that Sutton [makes](https://www.dwarkesh.com/p/richard-sutton) and I agree with him on this.

To motivate this, I want to draw an example from our recent AI past: [Move 37](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol). Move 37 was a move played by AlphaGo in its historic match against Lee Sedol in 2016. Move 37 was a move that no human would make (I don't know enough about Go to say more) and AlphaGo ended up winning the game. I think it's fairly safe to argue that a model trained only using next-token prediction on human games likely wouldn’t invent it. Action mattered. This is a fairly simple example someone could test (does next token prediction on human Go moves ever discover new moves?).

LLM-based digital agents now all have some RL on top of it, but it's probably not enough. They are still burdened with tHe represnetations they learned during pretraining.  They are not good representations for action. As already mentioned in the above sections (and detailed thoroughly by Anthropic) they are, in fact, particularly bad representations for acting. They are entangled. 

### Good representations are actionable

A real brain is not given data. It's housed inside a body and all sense data from the world is captured while moving through it. Hold your self as still as possible and you will notice that it is impossible. Your eyes move from word to word as you read this. So, the first thing you need to learn in the world is how to disentangle the self from all the input data. 

That is essential for intelligence. To be able to learn a world model and simulate counterfactual universes from it, you need to be able to disentangle the self from the world. Further, you ened to be able to infer what in the world will change when you take an action. This is essentialy what Gibson meant when he said we perceive affordances.


Explicit hierarhcy naturally leads to [more disentangled representations](https://proceedings.neurips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html) when the data are truly hierarchical. There was a brief moment where the field cared a lot about disentanglement, but this was largely abandoned in favor of [untangling](https://pubmed.ncbi.nlm.nih.gov/17631409/) (i.e., finding linearly decodable concepts) [after training](https://transformer-circuits.pub/2022/toy_model/index.html#strategic-ways-out) rather than inducing disentanged representations (training models where individual dimensions interpretable). In general, it is very unlikely to get disentangled representations [without strong inductive biases](https://arxiv.org/abs/1811.12359).


This becomes a very important concept when you are an agent. Whether physical or digital, once you have action, signatures of your own actions are entangled in the data. To learn a good world model, you need to be able to disentangle the self from the world. This is essential for intelligence.

An action-conditioned state transition model is the essential building block for intelligence. In this sence the Friston camp is right. The [``universal generative model''](https://www.verses.ai/research-blog/why-learn-if-you-can-infer-active-inference-for-robot-planning-control) has this key ingredient. And often, it comes with hierarchy. Where it fails, is that the models are often linear generative models and these representations do not facilitate the learning of transformations or the preservation of relationships across levels or with action. And that is essential.