---
title: Recipe for Intelligence
date: 2025-9-30
description: A recipe for building representations that support intelligence
draft: true
---


## The Bittersweet Lesson

Richard Sutton's [*Bitter Lesson*](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) has been one of the most influential essays in AI. It distilled decades of experience into a simple, almost painful truth: the methods that scale best are the ones that **learn**, not the ones that **encode our intuitions**. Sutton argued that over time, hand-designed representations always lose to general-purpose learning and search.

But the *final paragraph* of *The Bitter Lesson* walks an internally contradictory tightrope that is hard to make coherent. I've reproduced it in full here so you can see that I'm not misrepresenting it:

> "The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; **we should stop trying to find simple ways to think about the contents of minds**, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. **We want AI agents that can discover like we can**, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done." [emphasis is mine]

Sutton wants agents that *learn like we do*, yet insists we abandon any attempt to understand or embed the structures that make such learning possible. Now that Sutton is making the rounds arguing that [LLMs do not qualify as the Bitter Lesson](https://www.dwarkesh.com/p/richard-sutton), because they don't "learn like we do", it seems reasonable to take stock of what *like we do* means.

## Agents that learn "like we do"

The most concrete lesson neuroscience and psychology has to offer is that biological brains don't start *tabula rasa*. They have strong inductive biases. Different species build in different amounts of these inductive biases, which are encoded in the genome. Tony Zador has a nice review on this idea [here](https://www.nature.com/articles/s41467-019-11786-6), but in this blog post, I'm going to restrict "we" to humans.

So, if the *Bitter Lesson* was that hand-crafted content doesn't outperform general-purpose learning at scale, the **bittersweet lesson** is that *some structure must still be built in* for general purpose intelligence like ours. The challenge is to find the minimal ingredients that make intelligence both general and efficient.

In my view, there are three fundemental ingredients. I think all intelligence will have these three core features. These are necessary (and possibly sufficient) for representations that support intelligence. I introduce them in a particular order because they build on each other in sequence:

- **Geometry** (Representation of Transformations)

- **Hierarchy** (Coarse Graining)

- **Action** (Learning through Doing)

Of course hierarchy is familiar to anyone who has worked with deep learning. And action is familiar to anyone in RL. But I'm going to make these definitions more precise below and suggest ways in which they interact. Importantly, LLMs already have aspects of these ingredients, but they need all three.

## Geometry reduces the space of solutions and facilitates generalization

Geometry isn't really about shapes. It is about how things relate to each other, and the transformations that preserve those relationships.

- Rotate a square, and it's still a square.

- Transpose a melody into a higher key, and it's still the same tune.

- Move an object and all its parts move in relation to each other.

Here is the working definition of geometry I will use here: the relational structure of things and the transformations that preserve that structure.

In modern deep learning, this is typically the domain of *equivariant* neural networks. Equivariance means that transformations in the inputs have corresponding transformations in the representations. For example, convolutional neural networks are translation equivariant (at least up to [aliasing artifacts](https://arxiv.org/abs/1904.11486)). If you translate the input image, the representations are translated the same way. Geometric deep learning extends this to other transformations like rotations, scalings, and more abstract ones like changes in lighting or style.

There's good reason to pursue this direction. Geometry has a universal quality to it. Ancient cultures discovered geometry independently: Euclid in Greece, Liu Hui in China, the Sulba Sutras in India. They also discovered music scales built on frequency ratios. Why? Because the world is structured around such invariances, and brains are especially good at discovering them.

Some animal brains have geometry literally built in: flies represent head direction in a [literal ring of neurons](https://www.simonsfoundation.org/2017/08/01/in-fruit-fly-brain-a-ring-for-navigation/). Others are able to learn it quickly. Humans and chicks can generalize from one modality to another with very little training. [Molyneux's problem](https://en.wikipedia.org/wiki/Molyneux%27s_problem) asks whether a person born blind, upon gaining sight, could recognize shapes they had only known by touch. The empirical answer is basically "yes"! With very little experience, people with restored vision can [match across senses](https://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Molyneux_NatureNeuro2011.pdf). Chickens [solve versions](https://royalsocietypublishing.org/doi/10.1098/rsbl.2024.0025) of Molyneux's problem instantly.
 <!-- Neuroscientists think working memory uses [ring attractors](https://www.pnas.org/doi/10.1073/pnas.2210622120) and cognition leverages [codes for physical space](https://www.sciencedirect.com/science/article/pii/S0896627318308560?utm_source=chatgpt.com). -->

The lesson is that vision, touch, movement, and thought can align because they share relational structure. Geometry provides a common representational format across modalities.

This is where today's LLMs fall short. They do exhibit some relational structure (king–man+woman≈queen), but many features live in [superposition](https://transformer-circuits.pub/2023/monosemantic-features/index.html), entangled in shared dimensions rather than cleanly factorized. That makes transformations brittle and [context-dependent](https://transformer-circuits.pub/2025/attribution-graphs/methods.html). A concrete example comes from video models like Sora: one of the main advances in Sora 2 was ["stronger frame consistency"](https://dev.to/alifar/sora-2-next-generation-text-to-video-ai-explained-acl). But why was consistency a problem in the first place? Because the underlying representations weren't biased toward stability under transformation. Geometry is precisely about this kind of stability. The right inductive bias makes consistency default instead of something that requires training on the whole internet.

At their worst, models without geometry resemble Borges's [Funes the Memorious](https://www.nature.com/articles/475453a). The titular character has a sort of brain damage where he remembers everything in excruciatingly particular detail but is unable to grasp abstractions or maintain stable representations as they transform over time and space: Funes experiences the mane of a horse as a constantly changing flame, in contrast to our intuition of a [simple geometric shape](https://en.wikipedia.org/wiki/Funes_the_Memorious). This is exactly what happened in early video models. One solution is more data. Another is to *learn like we do* and build the right inductive biases into the model.

### The limits of geometric formalism

It's important not to take the geometry metaphor too far -- and I think equivariant neural networks does. The mathematical framework of geometric deep learning — where representations are strictly closed under group actions — is an idealization. Biological representations do not follow that idealized form.

Consider the [Thatcher effect](https://en.wikipedia.org/wiki/Thatcher_effect). When a face is inverted and the eyes and mouth are flipped right-side up within it, the result looks grotesque when the whole image is turned upright — but appears surprisingly normal when viewed upside down. If face representations were strictly equivariant under rotation, inverting the whole image would simply invert the representation, and the local manipulations would be just as salient either way. They aren't. The brain's representation of faces is heavily orientation-dependent, tuned to the statistics of upright viewing, and breaks in informative ways when that assumption is violated.

<img src="/intelligencerecipe/thatcher_illusion.avif" class="" style="max-width: min(100%, 48em)" alt="Thatcher erffect" />

The point is that biological representations are more flexible. Representations are approximately invariant under common transformations, but the approximation degrades for transformations the organism rarely encounters. This means the system allocates representational precision where it matters most. Strict group-theoretic closure is a useful mathematical framework, but I suspect real intelligence will be more flexible — strong geometric biases shaped by the statistics of experience and the demands of behavior.

## Hierarchy goes two ways

Hierarchy is primarily about coarse-graining. Coarse-graining means throwing away detail to focus on structure at a larger scale. In physics, you might ignore individual molecules and describe only temperature and pressure. In intelligence, hierarchy works the same way: pixels become edges, edges become shapes, shapes become objects, objects become categories.

But true hierarchy isn't just one-way compression. Once you've formed a high-level interpretation, the details have to fit it. If you decide you're looking at a coffee cup, the handle, rim, and shading all snap into place. This is how inference actually works: high-level concepts reshape how low-level evidence is understood. Without it, abstraction floats unanchored from the data.

Deep learning captures part of this. Deeper layers of CNNs and transformers do become more abstract. But the flow is only forward. The model doesn't settle on an interpretation and then revise the details to be consistent with it. Hierarchy in brains is different. For every feedforward connection, there is a feedback connection. And these feedback connections are not merely modulatory — they reshape early representations. [Lee and Mumford (2003)](https://link.springer.com/article/10.1023/A:1023983005329) argued that recurrent processing between levels of the visual hierarchy is essential for resolving ambiguity: early visual areas don't just pass information up; they receive interpretations back down, and their activity changes as a result. An edge that is ambiguous in isolation becomes a contour when the higher level has settled on an object. This is what makes biological hierarchy genuinely bidirectional, not just deep.

I've pointed out in the past how much [inference will look like feedforward processing](https://jake.vision/blog/inside-out-perspective) in most conditions. But overall, perception toes the line. Low-level details adjust to fill in.

Geometry and hierarchy work together. Think about zooming out on Google Maps: the street names and details disappear and more abstract concepts like pedestrian zones become visible. But now move the map and zoom back in and all the details are still there in the right place. That is geometry (relationships preserved under transformation) joined with hierarchy (abstraction across scales). If those two don't fit together, details wander off when concepts move.

## Action
Geometry and hierarchy alone can give us structured representations, but only action can make them causal. Acting is the only way to discover causal structure [importance of action](https://sergeylevine.substack.com/p/sporks-of-agi). This, I'd argue, is widely appreciated by the RL community, but often not given enough emphasis. We humans learn and think through [action](https://dl.acm.org/doi/10.1145/3325480.3325525). Sutton's main argument against LLMs is that action is not built in to their [objective -- "they have no goal"](https://www.dwarkesh.com/p/richard-sutton)

But action creates a deep representational problem: it entangles the self with the world. Every time you act, the sensory data reflect both the world’s response and your own movements. To be intelligent, a system must disentangle self from world. Think about a camera mounted to a robot's head. As the robot moves through space, it picks up motion signatures that have nothing to do with the world itself. The brain must separate these out. This is where all of a sudden disentanglement actually becomes important. 

There was a brief moment where the field cared a lot about disentanglement, but this was largely abandoned in favor of [untangling](https://pubmed.ncbi.nlm.nih.gov/17631409/) (i.e., finding linearly decodable concepts) [after training](https://transformer-circuits.pub/2022/toy_model/index.html#strategic-ways-out) rather than inducing disentanged representations (training models where individual dimensions interpretable). One thing that came of this brief interest in disentanglement is we know it is very unlikely to get disentangled representations [without strong inductive biases](https://arxiv.org/abs/1811.12359). Interestingly, hierarchy naturally leads to [more disentangled representations](https://proceedings.neurips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html) when the data are truly hierarchical (as in the real world). Yet despite their depth, LLMs are [totally entangled](https://www.anthropic.com/research/mapping-mind-language-model).

Imagine now, trying to build a causal model of the world on top of such an entangled representation. Sure, you can learn a model of transformations between frames and with a million hours of video, it will actually work [Genie](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/). But that's really not learning a world model *like we do*. We learn to perceive the world in terms of action. This is what James Gibson meant by [“affordances”](https://en.wikipedia.org/wiki/Affordance). He argued that perception encodes what the object offers the agent, or in my words, what will happen if it acts on it. Representations should reflect a probability distribution over outcomes given actions. Thus, useful representations are *actionable*. And that means they must disentangle what belongs to the agent from what belongs to the environment.

A natural way to formalize the concept of a world model is with an action-conditioned state-space model. For example, in Friston’s active inference framework, this action conditioned state space model is a parially observed Markov decision process and it forms the [universial generative model]((https://www.verses.ai/research-blog/why-learn-if-you-can-infer-active-inference-for-robot-planning-control)) used in virtually all of his papers. The generative model has two key components: The observation model links hidden states to evidence and the transition prior predicts how states evolve given actions and the past. Both are essential for inference. But the transition prior is where action and dynamics enter the model. It encodes the causal structure of how actions change the state of the world and also the dynamics of the self-world in. This is where Geometry and Hierarchy need to go. Where the action is.

Geometry ensures the world remains stable under transformation. And actions are themselves among the most common transformations we apply to the world: move your eyes, reach for an object, turn your head. Learning geometry is essential for grounding transformations that result from the agent’s own behavior. Hierarchy then allows this disentangling to happen across scales: self-motion vs object motion. This extends to more abstract actions like code edits. If I edit a program, my actions modify individual lines of code, but the consequences are on teh function of the program itself. Thus a full understanding of the codebase has an actionable representtion of what the consequences of edits will be.

And this is where geometry and hierarchy matter most. The rest of the state space model can be pretty flexible. Without inductive biases that favor relational structure and multi-scale abstraction, the transition prior is underconstrained. Of course, this *could* be solved with scale. You *could* try to brute force a function approximation of what this transition model should be. But this takes more data than exists. It's incredibly sample inefficient. Take $$\pi_0$$ [for example](https://www.physicalintelligence.company/download/pi0.pdf). π₀ was trained on 10,000 hours of robot demonstration data used for embodied pretraining following a Vision-Language Model (VLM) that was already pretrained on Internet-scale image–text data. This is, frankly, crazy. And not at all an agent that *learns like we do*. 

## The Transition Prior: Where Learning Meets the World

If geometry and hierarchy give structure to representation, **action** gives it purpose. Action is how prediction becomes understanding—how correlations become causes. The moment an agent acts, it reveals which parts of the world move with it and which move on their own. That’s the start of a causal model.

In formal terms, this lives in the **transition prior** of a state-space model:  
$$p(s_{t+1} \mid s_t, a_t)$$
It’s the one place where the self meets the world. All the complexity of perception, embodiment, and control ultimately flows through this model. That’s where geometry, hierarchy, and action converge—where the agent learns the structure of transformations, the abstractions that persist across scales, and the causal regularities that tie them to behavior.

Learning to act is, in the end, the only way to learn a world model. It’s also the only way to make intelligence efficient. So if we’re looking for a recipe, that’s where to start: **a transition prior that encodes the geometry of the world and disentangles the geometry of the self.**