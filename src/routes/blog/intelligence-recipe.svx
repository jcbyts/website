---
title: Recipe for Intelligence
date: 2025-9-30
description: A recipe for building representations that support intelligence
draft: true
---

## The Bittersweet Lesson

Richard Sutton’s [*Bitter Lesson*](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) has been one of the most influential essays in AI. It distilled decades of experience into a simple, almost painful truth: the methods that scale best are the ones that **learn**, not the ones that **encode our intuitions**. Sutton argued that over time, hand-designed representations always lose to general-purpose learning and search.

But the *final paragraph* of *The Bitter Lesson* walks a internally contradictory tightrope that is hard to make coherent:

> “The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; **we should stop trying to find simple ways to think about the contents of minds**, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. **We want AI agents that can discover like we can**, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.” [emphasis is mine]

Sutton wants agents that *learn like we do*, yet insists we abandon any attempt to understand or embed the structures that make such learning possible. Now that Sutton is making the rounds arguing that [LLMs do not qualify as the Bitter Lesson](https://www.dwarkesh.com/p/richard-sutton), because they don't "learn like we do", it seems reasonable to take stock of what *like we do* means.

## Agents that learn "like we do"

The most striking and concrete lesson neuroscience and psychology has to offer is that biological brains don’t start blank. They have strong inductive biases. If we want agents that learn like we do, these priors aren’t obstacles to generality—they’re its enablers.

So, if the *Bitter Lesson* was that hand-crafted content doesn’t scale, the **bittersweet lesson** is that *some structure must still be built in*: the kind that reflects the lawful geometry of the world and the embodied dynamics of the learner. The challenge is to find the minimal, compositional ingredients that make intelligence both general and efficient.

In my view, there are three. They form a kind of recipe. I think all intelligence will have these three core features. These are necessary (and possibly sufficient) for represnetations that support intelligence.  I introduce these in a particular order because they build on each other in sequence:

-**Geometry** (Representation of Transformations)

-**Hierarchy** (Coarse Graining)

-**Action** (Active inference)

Of course Hierarchy is familiar to anyone who has worked with deep learning. And action is familiar to anyone in RL. But I'm going to make these definitions more precise below and suggest ways in which these things interact. Importantly, LLMs already have aspects of these ingredients, but they need all three.

## Geometry reduces the space of solutions and facilitates generalization

Geometry isn’t really about shapes. It is about how things relate to each other, and the transformations that preserve those relationships.

- Rotate a square, and it’s still a square.

- Transpose a melody into a higher key, and it’s still the same tune.

- Move an object and all its parts move in relation to each other.

Here is the working definition of geometry I will use here: the relational structure of things and the transformations that preserve that structure. Both the relationships and the transformations may be learned from data, but biological brains are biased to find such structure.

Ancient cultures discovered geometry independently: Euclid in Greece, Liu Hui in China, the Sulba Sutras in India. They also discovered music scales built on frequency ratios. Why? Because the world is structured around such invariances, and brains are especially good at discovering them.

Animal brains have geometry built in. Flys represent head direction in a [literal ring of neurons](https://pubmed.ncbi.nlm.nih.gov/28473639/) and mice [represent this geometry](https://www.nature.com/articles/s41593-019-0460-x) with networks of neurons that are not spatially arranged. Humans and chicks can generalize from one modality to another with very little training. [Molyneux’s problem](https://en.wikipedia.org/wiki/Molyneux%27s_problem) asks whether a person born blind, upon gaining sight, recognize shapes they had only known by touch? The empirical answer is basically "yes"! With very little experience, people with restored vision can [match across senses](https://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Molyneux_NatureNeuro2011.pdf). Chickens [solve versions](https://royalsocietypublishing.org/doi/10.1098/rsbl.2024.0025) of Molyneux's problem instantly. Neuroscientists think working memory uses [ring attractors](https://www.pnas.org/doi/10.1073/pnas.2210622120) and cognitition leverages [codes for physical space](https://www.sciencedirect.com/science/article/pii/S0896627318308560?utm_source=chatgpt.com). 

The lesson is that vision, touch, movement, and thought can align because they share relational structure. Geometry provides a common representational format across modalities.

This is where today’s LLMs fall short. They do exhibit some relational structure (king–man+woman≈queen), but many features live in [superposition](https://transformer-circuits.pub/2023/monosemantic-features/index.html), entangled in shared dimensions rather than cleanly factorized. That makes transformations brittle and [context-dependent](https://transformer-circuits.pub/2025/attribution-graphs/methods.html). A concrete example comes from video models like Sora: one of the main advances in Sora 2 was ["stronger frame consistency"](https://dev.to/alifar/sora-2-next-generation-text-to-video-ai-explained-acl). But why was consistency a problem in the first place? Because the underlying representations weren’t biased toward stability under transformation. Geometry is precisely about this kind of stability. The right inductive bias makes consistency the default, not a patch.

At their worst, models without geometry resemble Borges’s [Funes the Memorious](https://www.nature.com/articles/475453a). The titular character has a sort of brain damage where he remembers everything in excruciatingly particular detail but is unable to grasp abstract or maintain stable representations as they transform over time and space: Funes experiences the mane of a horse as a constantly changing flame that is in contrast to our (normal people's) [intuition of a simple geometric shape](https://en.wikipedia.org/wiki/Funes_the_Memorious). This is exactly what happenened in early video models. One solution is more data. Another is to *learn like we do* and build the right inductive biases into the model.


## Hierarchy in both directions

Hierarchy is primarily about coarse-graining. Coarse-graining means throwing away detail to focus on structure at a larger scale. In physics, you might ignore individual molecules and describe only temperature and pressure. In intelligence, hierarchy works the same way: pixels become edges, edges become shapes, shapes become objects, objects become categories.

But true hierarchy isn’t just one-way compression. Once you’ve formed a high-level interpretation, the details have to fit it. If you decide you’re looking at a coffee cup, the handle, rim, and shading all snap into place. This is how true inference works: high-level concepts reshape how low-level evidence is understood. Without it, abstraction floats unanchored from the data.

Deep learning captures part of this. Deeper layers of CNNs and transformers do become more abstract. But the flow is only forward. The model doesn’t settle on an interpretation and then revise the details to be consistent with it. Hierarchy in brains is different. For every feedforward connection, there is a feedback connection. And there is a mountain of neural and behavior evidence supporting the idea that feedback connections are essential for some of the more conginitive aspects of intelligence and perception. Basically, perception tows the line. Low level details adjust to fill in.

Geometry and hierarchy work together. Think about zooming out on Google maps, the streetnames and details disappear and more abstract concepts like pedestrian zones become visible. But now move the map and zoom back in and all the details are still there in the right place. That is geometry (relationships preserved under transformation) joined with hierarchy (abstraction across scales). If those two don’t fit together, details wander off when concepts move. This is the same problem we saw in early video models: when an object moves, its surface features don’t reliably move with it. A true hierarchy keeps details tied to the whole.

This kind of bidirectional consistency isn’t optional. Without hierarchy, you drown in particulars like Borges’s Funes. Without geometry, abstractions and details fall apart. With both, you get a structured world model where fine and coarse levels constrain each other.

## Action
Geometry and hierarchy alone can give us structured representations, but only action can make them causal. Acting is the only way to discover causal structure [importance of action](https://sergeylevine.substack.com/p/sporks-of-agi). This, I'd argue, is widely appreciated by the RL community, but often not given enough emphasis. We humans learn and think through [action](https://dl.acm.org/doi/10.1145/3325480.3325525). Sutton's main argument against LLMs is that action is not built in to their [objective -- "they have no goal"](https://www.dwarkesh.com/p/richard-sutton)

To motivate how important this, let's revisit our recent AI past: AlphaGo. In 2016, AlphaGo shocked the Go world when beat Lee Sedol. In game 2, it played a move that no human would’ve ever made, ["move 37"](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol). Professional commentators initially thought it was a mistake. But it wasn’t and AlphaGo went on to win the game (and change [how humans play the game](https://www.pnas.org/doi/10.1073/pnas.2214840120)). Could a model trained only to imitate human play have generated such a move? Almost certainly not. A purely predictive model doesn’t know what would happen if it tries something new. Action makes that possible.

But action creates a deep representational problem: it entangles the self with the world. Every time you act, the sensory data reflect both the world’s response and your own movements. To be intelligent, a system must disentangle self from world. Think about a camera mounted to a robot's head. As the robot moves through space, it picks up motion signatures that have nothing to do with the world itself. The brain must separate these out.

Explicit hierarchy naturally leads to [more disentangled representations](https://proceedings.neurips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html) when the data are truly hierarchical. There was a brief moment where the field cared a lot about disentanglement, but this was largely abandoned in favor of [untangling](https://pubmed.ncbi.nlm.nih.gov/17631409/) (i.e., finding linearly decodable concepts) [after training](https://transformer-circuits.pub/2022/toy_model/index.html#strategic-ways-out) rather than inducing disentanged representations (training models where individual dimensions interpretable). In general, it is very unlikely to get disentangled representations [without strong inductive biases](https://arxiv.org/abs/1811.12359).

Disentanglement begins to matter a lot once you include action.

Technically, the natural way to formalize this is with an action-conditioned state-space model. For example, in Friston’s active inference framework, this is the [universal generative model](https://www.verses.ai/research-blog/why-learn-if-you-can-infer-active-inference-for-robot-planning-control). The generative model has two key components: The observation model links hidden states to evidence and the transition prior predicts how states evolve given actions. Both are essential for inference. But the transition prior is where action enters the model. It is where the self meets the world. It encodes the causal structure of how actions change the state of the world.

This is what James Gibson meant by [“affordances”](https://en.wikipedia.org/wiki/Affordance). He argued that perception encodes what the object offers the agent, or in my words, what will happen if it acts on it. Representations should reflect a probability distribution over outcomes given actions. Thus, useful representations are actionable. And that means they must disentangle what belongs to the agent from what belongs to the environment.

This is also where geometry and hierarchy re-enter. Geometry ensures the world remains stable under transformation. And actions are themselves among the most common transformations we apply to the world: move your eyes, reach for an object, turn your head. Learning geometry is essential for grounding transformations that result from the agent’s own behavior. Hierarchy then allows this disentangling to happen across scales: self-motion vs object motion. This extends to more abstract actions like code edits. If I edit a program, my actions modify individual lines of code, but the consequences are on teh function of the program itself. Thus a full understanding of the codebase has an actionable representtion of what the consequences of edits will be.

And this is where geometry and hierarchy matter most. Without inductive biases that favor relational structure and multi-scale abstraction, the transition prior cannot disentangle self from world, and it cannot link detailed actions to high level concepts. This is why all three ingredients are necessary for true intelligence. In a future post, I'll spell out how these might all converge in the transition prior of a state space model.

## The Transition Prior: Where Learning Meets the World

If geometry and hierarchy give structure to representation, **action** gives it purpose. Action is how prediction becomes understanding—how correlations become causes. The moment an agent acts, it reveals which parts of the world move with it and which move on their own. That’s the start of a causal model.

In formal terms, this lives in the **transition prior** of a state-space model:  
$$p(s_{t+1} \mid s_t, a_t)$$
It’s the one place where the self meets the world. All the complexity of perception, embodiment, and control ultimately flows through this model. That’s where geometry, hierarchy, and action converge—where the agent learns the structure of transformations, the abstractions that persist across scales, and the causal regularities that tie them to behavior.

Learning to act is, in the end, the only way to learn a world model. It’s also the only way to make intelligence efficient. So if we’re looking for a recipe, that’s where to start: **a transition prior that encodes the geometry of the world and disentangles the geometry of the self.**

That’s the real lesson—not bitter, not sweet, but *balanced*:  
General methods alone aren’t enough.  
Structure alone isn’t enough.  
Intelligence arises where **structure meets search**, **geometry meets action**, and **learning meets the world.**