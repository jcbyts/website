---
title: Recipe for Intelligence
date: 2025-9-30
description: A recipe for building representations that support intelligence
draft: true
---

## AGI or Stone Soup?

There's no denying that Large language models (LLMs) have been absolutely astonishing. Despite this, academics have largely denied that there is any intelligence there. Gary Marcus is the [loudest](https://fortune.com/2025/08/24/is-ai-a-bubble-market-crash-gary-marcus-openai-gpt5/) of the bunch, but its a fairly common stance: *whatever this is, it's not intelligence*. My favorite version of the critique is from Alison Gopnik, where she borrows the folktale of [Stone Soup](https://simons.berkeley.edu/news/stone-soup-ai) as a metaphor. The story involves a group of travelers who have no food and trick the villagers into providing all the ingredients for a delicious soup by saying they are making "stone soup", but of course it goes better with all the real food ingredients the villagers provide. The moral here is that LLMs only appear to be intelligent because they got the end product of our intelligence as the ingredients. They provided the stones and the cauldron and we poured in all the intelligence.

What is the right recpie for intelligence? What ingredients do we need? In contrast to the academics, experts on the industry side claim AGI "takeoff" might happen as soon as [2027](https://ai-2027.com/) and there is widespread faith in [the Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). But now even Richard Sutton -- Mr. Bitter Lesson himself -- has joined the naysayers insiting that LLMs [can't get there](https://garymarcus.substack.com/p/game-over-for-pure-llms-even-turing). His key ingredient is action, and it's one of my list as well. 

No matter how you slice it, we've run out of data, next-token prediction plus RL is not quite there. I think there are three key ingredients that are facilitate the learning of representations that support intelligence. I think any system that has these ingredients will be capable of intelligence.

## The Ingredients

In this post, I'm going to focus on the ingredients, because there are probably many ways to combine them. I think all intelligence will have these three core features. These are necessary (and possibly sufficient) for represnetations that support intelligence.  I introduce these in a particular order because they build on each other in sequence:

-**Geometry** (Representation of Transformations)

-**Hierarchy** (Coarse Graining)

-**Action** (Active inference)

Of course Hierarchy is familiar to anyone who has worked with deep learning. And action is familiar to anyone in RL. I'm going to make these definitions more precise below and suggest ways in which these things interact.

## Geometry: A Rosetta Stone for Thought
Geometry isn’t really about shapes. It is about how things relate to each other and the preservation of those relationships under certain transformations.

- Rotate a square, and it’s still a square.

- Transpose a melody into a higher key, and it’s still the same tune.

- Move an object and all its parts move in relation to eachother.

This is my working definition of geometry: the relational structure of things and the transformations that preserve that structure.

This is *central* to understanding the world. And it is why geometry is the closest thing we have to a universal language. Ancient cultures discovered geometry independently: Greece (Euclid), China (Liu Hui), India (Sulba Sutras). Similarly, they discovered music theory and scales as frequency ratios. Why? Because it is a universal description of spacetime in the world we experience. 

These concepts so ubiquitious it seems possible we have geometric schemas we use for thought. In the 1600s, William Molyneux posed a question about whether a blind person who suddenly gains sight would be able to match a visual stimulus to a familiar tactile one. Does the learned geometry of touch generalize to vision? Amazingly, there are empirical answers to this now. People who were blind from birth but recently gained sight (through cataract surgery) were able to learn to link visual experience to objects they had previously only experienced through touch in very little [experience](https://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Molyneux_NatureNeuro2011.pdf). Similarly, chickens [solve Molnyneux's problem](https://royalsocietypublishing.org/doi/10.1098/rsbl.2024.0025). Geometry, then, becomes a Rosetta stone for the different parts of the brain. Vision, touch, and movement don’t share raw signals, but they can all agree on relational structure.

Now, I am not claiming that geometry is explicitly hard coded, but whatever architecture is used to train intelligence must facilitate the learning of transformations. Technically, this is called [equivariance](https://en.wikipedia.org/wiki/Equivariant_map). In the extreme, this might be explicitely represented: like in the way the convolutions in a CNN are equivariant to [translations](https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59). This is the pursuit of [geometric deep learning](https://thegradient.pub/towards-geometric-deep-learning/). It is possible that geometric deep learning will prove the most fruitful, but I suspect something more relaxed will work as well. What is most important is that moving features around in latent space should preserve the relationships between them just as moving things around in the world should preserve the relationships between them.

LLMs don’t really capture geometry like I mean here. We do see linear substructure and feature subspaces (e.g., “king–man+woman≈queen”-style structure generalized to transformers). And, of course, all the training data has relational structure, but many features live in superposition—entangled in shared dimensions rather than cleanly factorized. That makes transformations brittle and context-dependent rather than preserved. In neuroscience, this can be [a good thing](https://www.nature.com/articles/nature12160), but it is brittle. This is also why some researchers call LLMs paradoxically under-parameterized. Not because they’re too small — they have billions of weights — but because the way they represent concepts doesn’t carve out stable dimensions for preserving relationships. They are underparameterized because they have to represent every instance of a concept, rather than the concept and transformations of it. 

At their worst, LLMs suffer just like Borges's "Funes the Memorious". In "Funes the Memorious", The titular character has a sort of brain damage where he remembers everything in excruciatingly particular detail but is unable to grasp abstract ideas. A dog seen at noon and later in the day are two different things to Funes. If this is your reality, with no ability to represent transformations, you can't have intelligence. LLMs are not this bad, but it's important to highlight my use of the word "geometry". It is about architectures that preserve relationships under (certain) transformations. This will be the glue that holds the other two ingredients together.

## Hierarchy (Coarse Graining)

Intelligence isn’t about recording detail. In fact, it's generally the opposite, it's about throwing away detail, abstracting. The way we solve this is with hierarchy: compressing fine-grained signals into coarser, more abstract ones. Hierarchical abstraction is essential for reasoning. When deciding which college to attend or job to take, you don’t simulate every sock you’ll wear. You think in broader abstractions. Similarly, in visual cortex, neurons at early stages respond to edges; higher stages combine those into shapes, then objects, then categories. 

Now wait a minute. Don't deep neural networks and LLMs already do this too? Isn't that what the "deep" in deep learning is all about? Yes, but not in the same way. The hierarchy in deep neural networks does get more abstract (e.g., object selectivity emerges in deep layers of CNNs). In fact, large LLMs have [emergent](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) geometry and hierarchy, but because they are not explicit, concepts are entangled within layers, and connections are brittle between. What is important is that the latent representations are cleanly glued together at at different levels of abstraction.

Think about zooming out on Google maps, the streetnames and details disappear and more abstract concepts like pedestrian zones become visible. That is similar in the layers of a CNN or transformer. But now move the map and zoom back in and all the details are still there. That is hard with transformers or CNNs. Similarly, if I move a coffee cup from the table to my mouth, the image drawn on the side of the cup will move with it. Video models struggle with thi scenario. Sora 2 for example, promised ["stronger frame consistency"](https://dev.to/alifar/sora-2-next-generation-text-to-video-ai-explained-acl). I'm describing a hierarchy where frame consistency is the only option. For example, in [hierarchical inference](https://pubmed.ncbi.nlm.nih.gov/12868647/) explicitly has latents at each level that explain latent causes of 

LLMs, despite being “deep” networks, don’t quite have this kind of hierarchy. Tracking concepts between layers in transformer architectures is a sparse thread through a dense web of attention mechanisms. Change one concept and a different web appears. That is why it takes whole additional neural networks to understand how a concept flows through the levels of Claude.
 
And this is exactly where geometry and hierarchy intertwine: hierarchy without geometry doesn’t hold together. If the transformations aren’t consistent across levels, abstractions drift and lose meaning. The map you zoom out on at one level has to remain coherent when you zoom back in. 

So, part 2 of the recipe is hierarchy. And by that, I mean an architecture that facilitates the learning of abstractions while still obeying the geometry.

## Action (Active Inference)

Action is the most important but it makes more sense if I present the other two first. Intelligence isn’t prediction; it’s intervention and counterfactual reasoning. We need to understand consequences. We act to learn—move our eyes, ask questions, try an experiment. In fact, learning largely doesn't happen without [action](https://dl.acm.org/doi/10.1145/3325480.3325525). This is a point that Sutton [makes](https://www.dwarkesh.com/p/richard-sutton) and I agree with him on this. Intelligent action requires a model of the world, and a good model of the world requires action. 

To motivate this, I want to draw an example from our recent AI past: [Move 37](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol). Move 37 was a move played by AlphaGo in its historic match against Lee Sedol in 2016. Move 37 was a move that no human would make (I don't know enough about Go to say more) and Alpha Go ended up winning the game. An exciting result of new wildly unconventional moves, human Go performance, which had saturated started [improving](https://www.pnas.org/doi/epdf/10.1073/pnas.2214840120). I think it's fairly safe to argue that a model trained only using next-token prediction on human games likely wouldn’t invent it. Action mattered.

LLM-based digital agents now all have some RL on top of it, but it's probably not enough. They are still burdened with tHe represnetations they learned during pretraining.  They are not good representations for action. As already mentioned in the above sections (and detailed thoroughly by Anthropic) they are, in fact, particularly bad representations for acting. They are entangled. They are not compositional. 

Here, I think active inference has the right starting point. The fundamnetal representation that needs to be learned is an action-conditioned state transition model. From the get go, the network must learn how the latents change when actions are taken. This is the starting point for active inference. What happens next, how you select actions, whether there is a value function, etc. are all secondary. 

## Inferring the self. Good representations are actionable

A real brain is not given data. It's housed inside a body and all sense data from the world is captured while moving through it. Hold your self as still as possible and you will notice that it is impossible. Your eyes move from word to word as you read this. So, the first thing you need to learn in the world is how to disentangle the self from all the input data. That is essential for intelligence. To be able to learn a world model and simulate counterfactual universes from it, you need to be able to disentangle the self from the world. Further, you ened to be able to infer what in the world will change when you take an action. This is essentialy what Gibson meant when he said we perceive affordances.

An action-conditioned state transition model is the essential building block for intelligence. In this sence the Friston camp is right. The [``universal generative model''](https://www.verses.ai/research-blog/why-learn-if-you-can-infer-active-inference-for-robot-planning-control) has this key ingredient. And often, it comes with hierarchy. Where it fails, is that the models are often linear generative models and these representations do not facilitate the learning of transformations or the preservation of relationships across levels or with action. And that is essential.

