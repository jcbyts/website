---
title: Recipe for Intelligence
date: 2025-9-30
description: A recipe for building representations that support intelligence
draft: true
---

## AGI or Stone Soup?

There's no denying that Large language models (LLMs) have been absolutely astonishing. Despite this, academics have largely denied that there is any intelligence there. Gary Marcus is the [loudest](https://fortune.com/2025/08/24/is-ai-a-bubble-market-crash-gary-marcus-openai-gpt5/) of the bunch, but its a fairly common stance: *whatever this is, it's not intelligence*. My favorite version of the critique is from Alison Gopnik, where she borrows the folktale of [Stone Soup](https://simons.berkeley.edu/news/stone-soup-ai) as a metaphor. The story involves a group of travelers who have no food and trick the villagers into providing all the ingredients for a delicious soup by saying they are making "stone soup", but of course it goes better with all the real food ingredients the villagers provide. The moral here is that LLMs only appear to be intelligent because they got the end product of our intelligence as the ingredients. They provided the stones and the cauldron and we poured in all the intelligence.

What is the right recpie for intelligence? What ingredients do we need? In contrast to the academics, experts on the industry side claim AGI "takeoff" might happen as soon as [2027](https://ai-2027.com/) and there is widespread faith in [the Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). But now even Richard Sutton -- Mr. Bitter Lesson himself -- has joined the naysayers insiting that LLMs [can't get there](https://garymarcus.substack.com/p/game-over-for-pure-llms-even-turing). His key ingredient is action, and it's one of my list as well. 

No matter how you slice it, we've run out of data, next-token prediction plus RL is not quite there. I think there are three key ingredients that facilitate the learning of representations that support intelligence. I think any system that has these ingredients will be capable of intelligence. I think without all three, we won't get there.

## The Ingredients

In this post, I'm going to focus on the ingredients, because there are probably many ways to combine them. I think all intelligence will have these three core features. These are necessary (and possibly sufficient) for represnetations that support intelligence.  I introduce these in a particular order because they build on each other in sequence:

-**Geometry** (Representation of Transformations)

-**Hierarchy** (Coarse Graining)

-**Action** (Active inference)

Of course Hierarchy is familiar to anyone who has worked with deep learning. And action is familiar to anyone in RL. But I'm going to make these definitions more precise below and suggest ways in which these things interact. Importantly, LLMs already have aspects of these ingredients, but they need all three.


## Geometry

Geometry isn’t really about shapes. It is about how things relate to each other, and the transformations that preserve those relationships.

- Rotate a square, and it’s still a square.

- Transpose a melody into a higher key, and it’s still the same tune.

- Move an object and all its parts move in relation to each other.

Here is the working definition of geometry I will use here: the relational structure of things and the transformations that preserve that structure. Both the relationships and the transformations should be learned from data, but the architecture should make that learning natural rather than fragile.

This idea shows up everywhere. Ancient cultures discovered geometry independently: Euclid in Greece, Liu Hui in China, the Sulba Sutras in India. They also discovered music scales built on frequency ratios. Why? Because the world is structured around such invariances, and brains are especially good at discovering them.

A striking demonstration comes from [Molyneux’s problem](https://en.wikipedia.org/wiki/Molyneux%27s_problem): could a person born blind, upon gaining sight, recognize shapes they had only known by touch? Thanks to modern science, we now know the answer is basically "yes"! With very little experience, people with restored vision can [match across senses](https://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Molyneux_NatureNeuro2011.pdf). Even chickens [solve versions](https://royalsocietypublishing.org/doi/10.1098/rsbl.2024.0025) of this problem. The lesson is that vision, touch, and movement don’t share raw signals, but they can align because they share relational structure. Geometry provides a common representational format across modalities.

For artificial systems, the technical term here is equivariance: representations should preserve relations when inputs are transformed. Convolutions in CNNs are equivariant to [translations](https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59). [Geometric deep learning](https://thegradient.pub/towards-geometric-deep-learning/) takes this principle further. But explicit equivariance may not always be required. The key point is that moving features around in latent space should preserve their relationships, just as moving things in the world preserves theirs.

This is where today’s LLMs fall short. They do exhibit some relational structure (king–man+woman≈queen), but many features live in [superposition](https://transformer-circuits.pub/2023/monosemantic-features/index.html), entangled in shared dimensions rather than cleanly factorized. That makes transformations brittle and [context-dependent](https://transformer-circuits.pub/2025/attribution-graphs/methods.html). A concrete example comes from video models like Sora: one of the main advances in Sora 2 was ["stronger frame consistency"](https://dev.to/alifar/sora-2-next-generation-text-to-video-ai-explained-acl). But why was consistency a problem in the first place? Because the underlying representations weren’t biased toward stability under transformation. Geometry is precisely about this kind of stability. The right inductive bias makes consistency the default, not a patch.

At their worst, models without geometry resemble Borges’s [Funes the Memorious](https://www.nature.com/articles/475453a). The titular character has a sort of brain damage where he remembers everything in excruciatingly particular detail but is unable to grasp abstract ideas. A dog seen at noon and later in the day are two different things to Funes. Without transformations, memory collapses into fragments. Geometry, in this sense, is the glue that holds representations together. It connects low-level details to higher-level abstractions, and it ensures that action on the world has predictable, stable consequences.

## Hierarchy

Hierarchy is primarily about coarse-graining. Coarse-graining means throwing away detail to focus on structure at a larger scale. In physics, you might ignore individual molecules and describe only temperature and pressure. In intelligence, hierarchy works the same way: pixels become edges, edges become shapes, shapes become objects, objects become categories.

But true hierarchy isn’t just one-way compression. Once you’ve formed a high-level interpretation, the details have to fit it. If you decide you’re looking at a coffee cup, the handle, rim, and shading all snap into place. This is iterative inference: high-level concepts reshape how low-level evidence is understood. Without it, abstraction floats unanchored from the data.

Deep learning captures part of this. Deeper layers of CNNs and transformers do become more abstract. But the flow is only forward. The model doesn’t settle on an interpretation and then revise the details to be consistent with it. That’s why I distinguish hierarchy in brains from “depth” in deep nets.

Geometry and hierarchy also have to work together. Think about zooming out on Google maps, the streetnames and details disappear and more abstract concepts like pedestrian zones become visible. But now move the map and zoom back in and all the details are still there in the right place. That is geometry (relationships preserved under transformation) joined with hierarchy (abstraction across scales). If those two don’t fit together, details wander off when concepts move. This is the same problem we saw in early video models: when an object moves, its surface features don’t reliably move with it. A true hierarchy keeps details tied to the whole.

This kind of bidirectional consistency isn’t optional. Without hierarchy, you drown in particulars like Borges’s Funes. Without geometry, abstractions and details fall apart. With both, you get a structured world model where fine and coarse levels constrain each other.

And hierarchy also matters because of disentanglement. Explicit hierarchy naturally leads to [more disentangled representations](https://proceedings.neurips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html) when the data are truly hierarchical. There was a brief moment where the field cared a lot about disentanglement, but this was largely abandoned in favor of [untangling](https://pubmed.ncbi.nlm.nih.gov/17631409/) (i.e., finding linearly decodable concepts) [after training](https://transformer-circuits.pub/2022/toy_model/index.html#strategic-ways-out) rather than inducing disentanged representations (training models where individual dimensions interpretable). In general, it is very unlikely to get disentangled representations [without strong inductive biases](https://arxiv.org/abs/1811.12359).

Disentanglement begins to matter a lot once you include action.

## Action
If geometry and hierarchy give us structured representations, action is what makes them causal. Prediction alone only gives correlations. Intelligence requires more: the ability to test interventions, to act and see what changes. Acting is the only way to discover causal structure. The [importance of action](https://sergeylevine.substack.com/p/sporks-of-agi) widely appreciated, but often not given enough emphasis. We learn and think through [action](https://dl.acm.org/doi/10.1145/3325480.3325525). Sutton himself rejects LLMs largely because action is not built in to their [objective](https://www.dwarkesh.com/p/richard-sutton)

To motivate how important this, let's revisit our recent AI past: AlphaGo. In 2016, AlphaGo shocked the Go world when beat Lee Sedol. In game 2, it played a move that no human would’ve ever made, ["move 37"](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol). Professional commentators initially thought it was a mistake. But it wasn’t and AlphaGo went on to win the game (and change [how humans play the game](https://www.pnas.org/doi/10.1073/pnas.2214840120)). Could a model trained only to imitate human play have generated such a move? Almost certainly not. A purely predictive model doesn’t know what would happen if it tries something new. Action makes that possible.

But action creates a deep representational problem: it entangles the self with the world. Every time you act, the sensory data reflect both the world’s response and your own movements. To be intelligent, a system must disentangle self from world. Think about a camera mounted to a robot's head. As the robot moves through space, it picks up motion signatures that have nothing to do with the world itself. The brain must separate these out.

Technically, the natural way to formalize this is with an action-conditioned state-space model. For example, in Friston’s active inference framework, this is the [universal generative model](https://www.verses.ai/research-blog/why-learn-if-you-can-infer-active-inference-for-robot-planning-control). The generative model has two key components: The observation model links hidden states to evidence and the transition prior predicts how states evolve given actions. Both are essential for inference. But the transition prior is where action enters the model. It is where the self meets the world. It encodes the causal structure of how actions change the state of the world.

This is what James Gibson meant by [“affordances”](https://en.wikipedia.org/wiki/Affordance). He argued that perception encodes what the object offers the agent, or in my words, what will happen if it acts on it. Representations should reflect a probability distribution over outcomes given actions. Thus, useful representations are actionable. And that means they must disentangle what belongs to the agent from what belongs to the environment.

This is also where geometry and hierarchy re-enter. Geometry ensures the world remains stable under transformation. And actions are themselves among the most common transformations we apply to the world: move your eyes, reach for an object, turn your head. Learning geometry is essential for grounding transformations that result from the agent’s own behavior. Hierarchy then allows this disentangling to happen across scales: self-motion vs object motion. This extends to more abstract actions like code edits. If I edit a program, my actions modify individual lines of code, but the consequences are on teh function of the program itself. Thus a full understanding of the codebase has an actionable representtion of what the consequences of edits will be.

And this is where geometry and hierarchy matter most. Without inductive biases that favor relational structure and multi-scale abstraction, the transition prior cannot disentangle self from world, and it cannot link detailed actions to high level concepts. This is why all three ingredients are necessary for true intelligence. In a future post, I'll spell out how these might all converge in the transition prior of a state space model.